{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "159c65b0",
      "metadata": {
        "id": "159c65b0"
      },
      "source": [
        "# Memory and Context Engineering for AI Agents with Oracle AI Database, Langchain and Tavily\n",
        "\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5340125d",
      "metadata": {
        "id": "5340125d"
      },
      "source": [
        "[![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=flat-square&logo=googlecolab)](https://colab.research.google.com/github/oracle-devrel/oracle-ai-developer-hub/blob/main/notebooks/memory_context_engineering_agents.ipynb)\n",
        "\n",
        "In this notebook, you'll learn how to engineer memory systems that give AI agents the ability to remember, learn, and adapt across conversations.\n",
        "Moving beyond simple RAG, we implement a complete **Memory Manager** with six distinct memory types‚Äîeach serving a specific cognitive function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56d6ba7",
      "metadata": {
        "id": "e56d6ba7"
      },
      "source": [
        "\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "| Memory Type | Purpose | Storage |\n",
        "|-------------|---------|---------|\n",
        "| **Conversational** | Chat history per thread | SQL Table |\n",
        "| **Knowledge Base** | Searchable documents & facts | Vector Store |\n",
        "| **Workflow** | Learned action patterns | Vector Store |\n",
        "| **Toolbox** | Dynamic tool definitions | Vector Store |\n",
        "| **Entity** | People, places, systems extracted from context | Vector Store |\n",
        "| **Summary** | Compressed context for long conversations | Vector Store |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65027f9",
      "metadata": {
        "id": "e65027f9"
      },
      "source": [
        "\n",
        "## Key Concepts Covered\n",
        "\n",
        "- **Memory Engineering**: Design patterns for agent memory systems\n",
        "- **Context Engineering**: Techniques for optimizing what goes into the LLM context\n",
        "- **Context Window Management**: Monitor usage, auto-summarize at thresholds\n",
        "- **Just-in-Time Retrieval**: Compact summaries with on-demand expansion\n",
        "- **Dynamic Tool Calling**: Semantic tool discovery and execution\n",
        "- **Entity Extraction**: LLM-powered entity recognition and storage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8775610",
      "metadata": {
        "id": "f8775610"
      },
      "source": [
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.10+\n",
        "- Oracle AI Database (local Docker or cloud)\n",
        "- OpenAI API key\n",
        "- Tavily API key\n",
        "\n",
        "## By the End\n",
        "You'll have a reusable `MemoryLayer` class and agent loop that demonstrates how modern AI agents maintain context, learn from interactions, and manage information across sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b243d181",
      "metadata": {
        "id": "b243d181"
      },
      "outputs": [],
      "source": [
        "! pip install -qU langchain-oracledb sentence-transformers langchain-openai langchain tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393bf345",
      "metadata": {
        "id": "393bf345"
      },
      "source": [
        "# Local Installation of Oracle AI Database via Docker [Memory Core]\n",
        "\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792a6485",
      "metadata": {
        "id": "792a6485"
      },
      "source": [
        "This section walks you through setting up **Oracle AI Database 26ai** locally using Docker. Oracle AI Database is a converged database that combines relational, document, graph, and vector data in a single engine‚Äîmaking it ideal for AI applications that need semantic search, embeddings storage, and vector similarity queries.\n",
        "\n",
        "**What you'll do:**\n",
        "1. Pull and run the Oracle Database Docker container\n",
        "2. Establish a connection from Python using `oracledb`\n",
        "3. Create a dedicated user for vector operations\n",
        "\n",
        "This local setup gives you a fully functional Oracle database for development and testing without needing cloud infrastructure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58676655",
      "metadata": {
        "id": "58676655"
      },
      "source": [
        "### Installing Oracle AI Database via Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c745a8",
      "metadata": {
        "id": "80c745a8"
      },
      "source": [
        "For this notebook we will be using a local installation of [Oracle AI Database](https://www.oracle.com/database/free/get-started/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c92b14d9",
      "metadata": {
        "id": "c92b14d9"
      },
      "source": [
        "1. Install & start Docker. Docker Desktop (Mac/Windows) or Docker Engine (Linux). Make sure it‚Äôs running.\n",
        "    - If installed with Docker Enginer, run from terminal ```open /Applications/Docker.app```\n",
        "2. We are going to pull the [docker image](https://container-registry.oracle.com/ords/f?p=113:4:13936724845291:::4:P4_REPOSITORY,AI_REPOSITORY,AI_REPOSITORY_NAME,P4_REPOSITORY_NAME,P4_EULA_ID,P4_BUSINESS_AREA_ID:1863,1863,Oracle%20Database%20Free,Oracle%20Database%20Free,1,0&cs=3cVNH02fFYhB723ODpNnr0JZI1S7Z64nRyL_zC1Ls5BSVLafGsOLMFvFoPhn8JeeB8tXPhkfFKH8-dkrL_z3_0g)\n",
        "3. Run a container with oracle image\n",
        "\n",
        "    ```\n",
        "      docker run -d \\\n",
        "        --name oracle-free \\\n",
        "        -p 1521:1521 -p 5500:5500 \\\n",
        "        -e ORACLE_PWD=OraclePwd_2025 \\\n",
        "        -v $HOME/oracle/full_data:/opt/oracle/oradata \\\n",
        "        container-registry.oracle.com/database/free:latest\n",
        "\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95d25a7",
      "metadata": {
        "id": "d95d25a7"
      },
      "source": [
        "> üö´ **Troubleshoot**  \n",
        "> If you see the error:  \n",
        "> *`docker: Error response from daemon: Conflict. The container name \"/oracle-full\" is already in use by container ... You have to remove (or rename) that container to be able to reuse that name.`*  \n",
        ">\n",
        "> üß© **Fix:**  \n",
        "> - Remove the existing container:  \n",
        ">   ```bash\n",
        ">   docker rm oracle-free\n",
        ">   ```  \n",
        "> - Then re-run your Docker command from **Step 3** to start a new container.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88463dc3",
      "metadata": {
        "id": "88463dc3"
      },
      "source": [
        "### üöÄ One-Click Database Setup\n",
        "\n",
        "The cell below handles **everything automatically**:\n",
        "- ‚úÖ Checks if Docker is running\n",
        "- ‚úÖ Checks if Oracle container exists and is healthy\n",
        "- ‚úÖ Waits for database to be ready (with progress indicator)\n",
        "- ‚úÖ Fixes the listener for ARM Macs (Apple Silicon)\n",
        "- ‚úÖ Creates the VECTOR user with proper privileges\n",
        "- ‚úÖ Tests the connection\n",
        "\n",
        "**Just run the cell below and wait for the ‚úÖ success message!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7ae8e6",
      "metadata": {
        "id": "fb7ae8e6"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import sys\n",
        "\n",
        "def setup_oracle_database(container_name=\"oracle-free\", vector_password=\"VectorPwd_2025\"):\n",
        "    \"\"\"\n",
        "    Complete Oracle Database setup - handles everything in one call.\n",
        "\n",
        "    This function:\n",
        "    1. Checks Docker is running\n",
        "    2. Verifies container exists and is healthy\n",
        "    3. Waits for database to be ready\n",
        "    4. Fixes listener for ARM Macs\n",
        "    5. Creates VECTOR user\n",
        "    6. Tests connection\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ ORACLE DATABASE SETUP\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Check Docker\n",
        "    print(\"\\n[1/6] Checking Docker...\")\n",
        "    try:\n",
        "        result = subprocess.run(['docker', 'info'], capture_output=True, text=True, timeout=10)\n",
        "        if result.returncode != 0:\n",
        "            print(\"   ‚ùå Docker is not running!\")\n",
        "            print(\"   üí° Start Docker Desktop and try again.\")\n",
        "            return False\n",
        "        print(\"   ‚úÖ Docker is running\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"   ‚ùå Docker not found! Please install Docker.\")\n",
        "        return False\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"   ‚ùå Docker is not responding. Please restart Docker.\")\n",
        "        return False\n",
        "\n",
        "    # Step 2: Check container\n",
        "    print(f\"\\n[2/6] Checking container '{container_name}'...\")\n",
        "    result = subprocess.run(\n",
        "        ['docker', 'ps', '-a', '--filter', f'name={container_name}', '--format', '{{.Status}}'],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    status = result.stdout.strip()\n",
        "\n",
        "    if not status:\n",
        "        print(f\"   ‚ùå Container '{container_name}' not found!\")\n",
        "        print(\"   üí° Run the docker run command from the previous cell first.\")\n",
        "        return False\n",
        "    elif \"Up\" not in status:\n",
        "        print(f\"   ‚ö†Ô∏è  Container exists but not running. Starting...\")\n",
        "        subprocess.run(['docker', 'start', container_name], capture_output=True)\n",
        "        time.sleep(5)\n",
        "\n",
        "    print(f\"   ‚úÖ Container is running\")\n",
        "\n",
        "    # Step 3: Wait for database ready\n",
        "    print(\"\\n[3/6] Waiting for database to be ready...\")\n",
        "    print(\"   (This can take 2-5 minutes on Apple Silicon Macs)\")\n",
        "\n",
        "    max_wait = 300  # 5 minutes\n",
        "    check_interval = 10\n",
        "    elapsed = 0\n",
        "\n",
        "    while elapsed < max_wait:\n",
        "        # Check container health\n",
        "        result = subprocess.run(\n",
        "            ['docker', 'ps', '--filter', f'name={container_name}', '--format', '{{.Status}}'],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        if \"healthy\" in result.stdout.lower():\n",
        "            print(f\"\\n   ‚úÖ Database is healthy!\")\n",
        "            break\n",
        "\n",
        "        # Also check logs for ready message\n",
        "        logs = subprocess.run(\n",
        "            ['docker', 'logs', '--tail', '20', container_name],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        if \"DATABASE IS READY TO USE\" in logs.stdout:\n",
        "            print(f\"\\n   ‚úÖ Database is ready!\")\n",
        "            break\n",
        "\n",
        "        # Progress indicator\n",
        "        dots = \".\" * ((elapsed // check_interval) % 4 + 1)\n",
        "        print(f\"\\r   ‚è≥ Waiting{dots.ljust(5)} ({elapsed}s elapsed)\", end=\"\", flush=True)\n",
        "        time.sleep(check_interval)\n",
        "        elapsed += check_interval\n",
        "    else:\n",
        "        print(f\"\\n   ‚ùå Timeout waiting for database. Check 'docker logs {container_name}'\")\n",
        "        return False\n",
        "\n",
        "    # Step 4: Fix listener (for ARM Macs)\n",
        "    print(\"\\n[4/6] Configuring listener...\")\n",
        "\n",
        "    # Fix listener.ora\n",
        "    subprocess.run(\n",
        "        ['docker', 'exec', container_name, 'bash', '-c',\n",
        "         \"sed -i 's/HOST = [^)]*)/HOST = 0.0.0.0)/g' /opt/oracle/product/26ai/dbhomeFree/network/admin/listener.ora\"],\n",
        "        capture_output=True\n",
        "    )\n",
        "\n",
        "    # Restart listener\n",
        "    subprocess.run(['docker', 'exec', container_name, 'lsnrctl', 'stop'], capture_output=True)\n",
        "    start_result = subprocess.run(\n",
        "        ['docker', 'exec', container_name, 'lsnrctl', 'start'],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "    if \"Listening on\" not in start_result.stdout:\n",
        "        print(\"   ‚ùå Failed to start listener\")\n",
        "        return False\n",
        "\n",
        "    # Register services\n",
        "    subprocess.run(\n",
        "        ['docker', 'exec', container_name, 'bash', '-c',\n",
        "         \"export ORACLE_SID=FREE && sqlplus -s / as sysdba <<< 'ALTER SYSTEM REGISTER;'\"],\n",
        "        capture_output=True\n",
        "    )\n",
        "    print(\"   ‚úÖ Listener configured and running\")\n",
        "\n",
        "    # Step 5: Create VECTOR user\n",
        "    print(\"\\n[5/6] Creating VECTOR user...\")\n",
        "\n",
        "    create_user_sql = f'''\n",
        "    DECLARE\n",
        "        user_count NUMBER;\n",
        "    BEGIN\n",
        "        SELECT COUNT(*) INTO user_count FROM all_users WHERE username = 'VECTOR';\n",
        "        IF user_count = 0 THEN\n",
        "            EXECUTE IMMEDIATE 'CREATE USER VECTOR IDENTIFIED BY {vector_password}';\n",
        "            EXECUTE IMMEDIATE 'GRANT CONNECT, RESOURCE, CREATE SESSION TO VECTOR';\n",
        "            EXECUTE IMMEDIATE 'GRANT UNLIMITED TABLESPACE TO VECTOR';\n",
        "            EXECUTE IMMEDIATE 'GRANT CREATE TABLE, CREATE SEQUENCE, CREATE VIEW TO VECTOR';\n",
        "            DBMS_OUTPUT.PUT_LINE('CREATED');\n",
        "        ELSE\n",
        "            DBMS_OUTPUT.PUT_LINE('EXISTS');\n",
        "        END IF;\n",
        "    END;\n",
        "    /\n",
        "    '''\n",
        "\n",
        "    result = subprocess.run(\n",
        "        ['docker', 'exec', container_name, 'bash', '-c',\n",
        "         f\"export ORACLE_SID=FREE && sqlplus -s / as sysdba <<< \\\"ALTER SESSION SET CONTAINER = FREEPDB1; {create_user_sql}\\\"\"],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "    if \"ORA-\" in result.stdout:\n",
        "        print(f\"   ‚ö†Ô∏è  Warning: {result.stdout}\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ VECTOR user ready\")\n",
        "\n",
        "    # Step 6: Test connection\n",
        "    print(\"\\n[6/6] Testing connection...\")\n",
        "    try:\n",
        "        import oracledb\n",
        "        conn = oracledb.connect(\n",
        "            user=\"VECTOR\",\n",
        "            password=vector_password,\n",
        "            dsn=\"127.0.0.1:1521/FREEPDB1\"\n",
        "        )\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(\"SELECT 1 FROM dual\")\n",
        "            cur.fetchone()\n",
        "        conn.close()\n",
        "        print(\"   ‚úÖ Connection successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Connection failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Success!\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üéâ SETUP COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\"\"\n",
        "You can now connect to Oracle:\n",
        "    User: VECTOR\n",
        "    Password: {vector_password}\n",
        "    DSN: 127.0.0.1:1521/FREEPDB1\n",
        "\"\"\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bacd1640",
      "metadata": {
        "id": "bacd1640",
        "outputId": "563937e8-76b4-4342-de70-180ac3488215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üöÄ ORACLE DATABASE SETUP\n",
            "============================================================\n",
            "\n",
            "[1/6] Checking Docker...\n",
            "   ‚ùå Docker is not running!\n",
            "   üí° Start Docker Desktop and try again.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run this cell after starting your Docker container\n",
        "# It handles everything: waits for ready, fixes listener, creates user, tests connection\n",
        "setup_oracle_database()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63b1afd2",
      "metadata": {
        "id": "63b1afd2"
      },
      "source": [
        "### Connection Helper Function\n",
        "\n",
        "In the code below we have a reusable function that connects to Oracle Database with automatic retry logic and helpful error messages.\n",
        "\n",
        "**What it does:**\n",
        "1. Attempts to connect using the `oracledb` Python driver\n",
        "2. Retries up to 3 times if the connection fails (useful when the database is still starting)\n",
        "3. Prints the Oracle version banner on successful connection. This will also include the version you are running\n",
        "4. Provides troubleshooting hints for common connection errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "322bb9ef",
      "metadata": {
        "id": "322bb9ef"
      },
      "outputs": [],
      "source": [
        "import oracledb\n",
        "import time\n",
        "\n",
        "def connect_to_oracle(max_retries=3, retry_delay=5, user=\"sys\", password=\"OraclePwd_2025\", dsn=\"127.0.0.1:1521/FREEPDB1\", program=\"langchain_oracledb_deep_research_demo\"):\n",
        "    \"\"\"\n",
        "    Connect to Oracle database with retry logic and better error handling.\n",
        "\n",
        "    Args:\n",
        "        max_retries: Maximum number of connection attempts\n",
        "        retry_delay: Seconds to wait between retries\n",
        "    \"\"\"\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            print(f\"Connection attempt {attempt}/{max_retries}...\")\n",
        "            conn = oracledb.connect(\n",
        "                user=user,\n",
        "                password=password,\n",
        "                dsn=dsn,\n",
        "                program=program\n",
        "            )\n",
        "            print(\"‚úì Connected successfully!\")\n",
        "\n",
        "            # Test the connection\n",
        "            with conn.cursor() as cur:\n",
        "                cur.execute(\"SELECT banner FROM v$version WHERE banner LIKE 'Oracle%';\")\n",
        "                banner = cur.fetchone()[0]\n",
        "                # Banner should include the version you are running\n",
        "                print(f\"\\n{banner}\")\n",
        "\n",
        "            return conn\n",
        "\n",
        "        except oracledb.OperationalError as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"‚úó Connection failed (attempt {attempt}/{max_retries})\")\n",
        "\n",
        "            if \"DPY-4011\" in error_msg or \"Connection reset by peer\" in error_msg:\n",
        "                print(\"  ‚Üí This usually means:\")\n",
        "                print(\"    1. Database is still starting up (wait 2-3 minutes)\")\n",
        "                print(\"    2. Listener configuration issue\")\n",
        "                print(\"    3. Container is not running\")\n",
        "\n",
        "                if attempt < max_retries:\n",
        "                    print(f\"\\n  Waiting {retry_delay} seconds before retry...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    print(\"\\n  üí° Try running: setup_oracle_database()\")\n",
        "                    print(\"     This will fix the listener and verify the connection.\")\n",
        "                    raise\n",
        "            else:\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Unexpected error: {e}\")\n",
        "            raise\n",
        "\n",
        "    raise ConnectionError(\"Failed to connect after all retries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8bacbe",
      "metadata": {
        "id": "1f8bacbe"
      },
      "source": [
        "Ensure you have your Docker Engine running before going through the next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03562c6",
      "metadata": {
        "id": "c03562c6"
      },
      "source": [
        "Connect as the `VECTOR` user dedicated schema for storing embeddings and vector data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3aa1368",
      "metadata": {
        "id": "f3aa1368"
      },
      "outputs": [],
      "source": [
        "vector_conn = connect_to_oracle(\n",
        "    user=\"VECTOR\",\n",
        "    password=\"VectorPwd_2025\",\n",
        "    dsn=\"127.0.0.1:1521/FREEPDB1\",\n",
        "    program=\"langchain_oracledb_deep_research_demo\",\n",
        ")\n",
        "\n",
        "print(\"Using user:\", vector_conn.username)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365bcafb",
      "metadata": {
        "id": "365bcafb"
      },
      "source": [
        "‚úÖ **Setup complete!** You now have Oracle AI Database running locally with an active connection.\n",
        "\n",
        "Next, we'll create vector stores using **LangChain's Oracle integration** to store embeddings and metadata for semantic search."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f8ccd5",
      "metadata": {
        "id": "47f8ccd5"
      },
      "source": [
        "# Vector Search With Langchain and Oracle AI Database\n",
        "\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69fa782a",
      "metadata": {
        "id": "69fa782a"
      },
      "source": [
        "This section demonstrates how to use **LangChain's Oracle Vector Store (OracleVS)** to store and search documents using semantic similarity.\n",
        "\n",
        "Vector search enables finding documents based on meaning rather than exact keyword matches.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "| Step | Description |\n",
        "|------|-------------|\n",
        "| **1. Initialize Embeddings** | Load a HuggingFace embedding model to convert text into vectors |\n",
        "| **2. Create Vector Store** | Set up an Oracle-backed vector store with distance strategy |\n",
        "| **3. Create Index** | Build an IVF (Inverted File) index for fast similarity search |\n",
        "| **4. Add Documents** | Store text with metadata in the vector database |\n",
        "| **5. Query** | Search for similar documents using natural language |\n",
        "| **6. Filter Results** | Use metadata filters to narrow down search results |\n",
        "\n",
        "## Key Components\n",
        "\n",
        "- **`OracleVS`**: LangChain's Oracle vector store integration\n",
        "- **`HuggingFaceEmbeddings`**: Converts text to 768-dimensional vectors\n",
        "- **`DistanceStrategy.EUCLIDEAN_DISTANCE`**: Measures similarity between vectors\n",
        "- **IVF Index**: Speeds up searches on large datasets by clustering vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e5cbeb",
      "metadata": {
        "id": "c9e5cbeb"
      },
      "source": [
        "## Creating Vector Stores with Langchain OracleVS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340bffc4",
      "metadata": {
        "id": "340bffc4"
      },
      "outputs": [],
      "source": [
        "from langchain_oracledb.vectorstores import OracleVS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_oracledb.vectorstores.oraclevs import create_index\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
        ")\n",
        "\n",
        "# Initialize the vector store\n",
        "vector_store = OracleVS(\n",
        "    client=vector_conn,\n",
        "    embedding_function=embedding_model,\n",
        "    table_name=\"VECTOR_SEARCH_DEMO\",\n",
        "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e24aa3",
      "metadata": {
        "id": "64e24aa3"
      },
      "outputs": [],
      "source": [
        "# Helper to safely create index (skips if already exists)\n",
        "def safe_create_index(conn, vs, idx_name):\n",
        "    \"\"\"Create index, skipping if it already exists.\"\"\"\n",
        "    try:\n",
        "        create_index(\n",
        "            client=conn,\n",
        "            vector_store=vs,\n",
        "            params={\"idx_name\": idx_name, \"idx_type\": \"IVF\"}\n",
        "        )\n",
        "        print(f\"  ‚úÖ Created index: {idx_name}\")\n",
        "    except Exception as e:\n",
        "        if \"ORA-00955\" in str(e):\n",
        "            print(f\"  ‚è≠Ô∏è Index already exists: {idx_name} (skipped)\")\n",
        "        else:\n",
        "            raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146cb8bc",
      "metadata": {
        "id": "146cb8bc"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "#¬†Suppress langchain_oracledb logging, remove this if you want to see the debug logs\n",
        "logging.getLogger(\"langchain_oracledb\").setLevel(logging.CRITICAL)\n",
        "\n",
        "# Create an IVF index for fast similarity search\n",
        "safe_create_index(vector_conn, vector_store, \"oravs_ivf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc1b4b9",
      "metadata": {
        "id": "8cc1b4b9"
      },
      "source": [
        "## Ingesting Example Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4926e267",
      "metadata": {
        "id": "4926e267"
      },
      "outputs": [],
      "source": [
        "# add texts to the vector database\n",
        "texts = [\"A tablespace can be online (accessible) or offline (not accessible) whenever the database is open.\\nA tablespace is usually online so that its data is available to users. The SYSTEM tablespace and temporary tablespaces cannot be taken offline.\", \"The database stores LOBs differently from other data types. Creating a LOB column implicitly creates a LOB segment and a LOB index. \"]\n",
        "metadata = [\n",
        "    {\"id\": \"100\", \"link\": \"Document Example Test 1\"},\n",
        "    {\"id\": \"101\", \"link\": \"Document Example Test 2\"},\n",
        "]\n",
        "\n",
        "# Simple Ingestion\n",
        "vector_store.add_texts(\n",
        "    texts=texts, # This is the text embeddings will be generated from\n",
        "    metadatas=metadata # This is the metadata that will be stored with the text\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1061c994",
      "metadata": {
        "id": "1061c994"
      },
      "source": [
        "## Querying the Vector Store\n",
        "\n",
        "Search for documents similar to a natural language query.\n",
        "\n",
        "The vector store converts queries to an embedding and finds the closest matches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78909ea2",
      "metadata": {
        "id": "78909ea2"
      },
      "source": [
        "Basic Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3196205a",
      "metadata": {
        "id": "3196205a"
      },
      "outputs": [],
      "source": [
        "query = \"How does Oracle handle tablespaces?\"\n",
        "\n",
        "results = vector_store.similarity_search(query, k=3)\n",
        "\n",
        "for i, doc in enumerate(results, start=1):\n",
        "    print(f\"--- Result {i} ---\")\n",
        "    print(\"Text:\", doc.page_content)\n",
        "    print(\"Metadata:\", doc.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ece922",
      "metadata": {
        "id": "99ece922"
      },
      "source": [
        "Search With Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e523e0",
      "metadata": {
        "id": "56e523e0"
      },
      "outputs": [],
      "source": [
        "results = vector_store.similarity_search_with_score(query, k=3)\n",
        "\n",
        "for doc, score in results:\n",
        "    print(\"Score:\", score)\n",
        "    print(\"Text :\", doc.page_content)\n",
        "    print(\"Meta :\", doc.metadata)\n",
        "    print(\"------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b10609d0",
      "metadata": {
        "id": "b10609d0"
      },
      "source": [
        "Filter by exact match on a metadata field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "081c7f74",
      "metadata": {
        "id": "081c7f74"
      },
      "outputs": [],
      "source": [
        "query = \"How are tablespaces made available to users?\"\n",
        "\n",
        "# This will only return docs where metadata.link == \"Document Example Test 1\".\n",
        "docs = vector_store.similarity_search(\n",
        "    query, k=3,\n",
        "    filter={\"link\": {\"$eq\": \"Document Example Test 1\"}},\n",
        ")\n",
        "\n",
        "for doc in docs:\n",
        "    print(\"Text:\", doc.page_content[:120], \"...\")\n",
        "    print(\"Meta:\", doc.metadata)\n",
        "    print(\"------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90290170",
      "metadata": {
        "id": "90290170"
      },
      "source": [
        "Filter by id list ($in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d8de15",
      "metadata": {
        "id": "e0d8de15"
      },
      "outputs": [],
      "source": [
        "docs = vector_store.similarity_search(\n",
        "    query=\"Explain database storage concepts\",\n",
        "    k=5,\n",
        "    filter={\"id\": {\"$in\": [\"100\"]}},  # only id 100\n",
        ")\n",
        "\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82623c94",
      "metadata": {
        "id": "82623c94"
      },
      "source": [
        "# Memory Engineering and Agent Memory\n",
        "--------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3ff4cf",
      "metadata": {
        "id": "7b3ff4cf"
      },
      "source": [
        "\n",
        "**`Agent Memory`** is the exocortex that augments an LLM‚Äîcapturing, encoding, storing, linking, and retrieving information beyond the model‚Äôs parametric and contextual limits.\n",
        "It provides the persistence and structure required for long-horizon reasoning and reliable behaviour.\n",
        "\n",
        "**`Memory Engineering`** is the scaffolding and control harness that we design to move information optimally and efficiently into, through, and across all components of an AI system(databases, LLMs, applications etc). It ensures that data is captured, transformed, organized, and retrieved in the right way at the right time‚Äîso agents can behave reliably, believably, and capabaly.\n",
        "\n",
        "This is the core section of the notebook where we build a complete **`Memory Manager`** for AI agents.\n",
        "\n",
        "Just like humans have different types of memory (short-term, long-term, procedural), AI agents benefit from specialized memory systems.\n",
        "\n",
        "## Why Memory Engineering Matters\n",
        "\n",
        "Without memory, agents:\n",
        "- Forget previous conversations\n",
        "- Can't learn from past interactions\n",
        "- Repeat the same mistakes\n",
        "- Lack context for complex tasks\n",
        "\n",
        "With proper memory engineering, agents can:\n",
        "- Maintain context across sessions\n",
        "- Learn and improve over time\n",
        "- Access relevant knowledge when needed\n",
        "- Execute complex multi-step workflows\n",
        "\n",
        "## Memory Types We'll Implement\n",
        "\n",
        "| Memory Type | Human Analogy | Purpose | Storage |\n",
        "|-------------|---------------|---------|---------|\n",
        "| **Conversational** | Short-term memory | Chat history per thread | SQL Table |\n",
        "| **Knowledge Base** | Long-term semantic memory | Facts, documents, search results | Vector Store |\n",
        "| **Workflow** | Procedural memory | Learned action patterns | Vector Store |\n",
        "| **Toolbox** | Skill memory | Available tools & capabilities | Vector Store |\n",
        "| **Entity** | Episodic memory | People, places, systems mentioned | Vector Store |\n",
        "| **Summary** | Compressed memory | Condensed context for long conversations | Vector Store |\n",
        "\n",
        "## Steps in This Section\n",
        "\n",
        "1. **Define table names** for each memory type\n",
        "2. **Create SQL table** for conversational history\n",
        "3. **Create vector stores** for semantic memories\n",
        "4. **Build indexes** for fast similarity search\n",
        "5. **Implement MemoryLayer class** with read/write methods for each memory type\n",
        "6. **Initialize the memory manager** with all storage backends"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461b4fa8",
      "metadata": {
        "id": "461b4fa8"
      },
      "source": [
        "## Define Memory Tables and Stores\n",
        "First, we define table names for each memory type.\n",
        "\n",
        "These tables will be created in Oracle Database to persist agent memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0ee2ee",
      "metadata": {
        "id": "8e0ee2ee"
      },
      "outputs": [],
      "source": [
        "# Table names for each memory type\n",
        "CONVERSATIONAL_TABLE   = \"CONVERSATIONAL_MEMORY\" # Episodic memory\n",
        "KNOWLEDGE_BASE_TABLE   = \"SEMANTIC_MEMORY\" # Semantic memory\n",
        "WORKFLOW_TABLE = \"WORKFLOW_MEMORY\" # Procedural memory\n",
        "TOOLBOX_TABLE    = \"TOOLBOX_MEMORY\" # Procedural memory\n",
        "ENTITY_TABLE = \"ENTITY_MEMORY\" # Semantic memory\n",
        "SUMMARY_TABLE = \"SUMMARY_MEMORY\" # Semanatic memory\n",
        "\n",
        "ALL_TABLES = [CONVERSATIONAL_TABLE, KNOWLEDGE_BASE_TABLE, WORKFLOW_TABLE, TOOLBOX_TABLE, ENTITY_TABLE, SUMMARY_TABLE]\n",
        "\n",
        "# Drop existing tables to start fresh\n",
        "for table in ALL_TABLES:\n",
        "    try:\n",
        "        with vector_conn.cursor() as cur:\n",
        "            cur.execute(f\"DROP TABLE {table} PURGE\")\n",
        "    except Exception as e:\n",
        "        if \"ORA-00942\" in str(e):\n",
        "            print(f\"  - {table} (not exists)\")\n",
        "        else:\n",
        "            print(f\"  ‚úó {table}: {e}\")\n",
        "\n",
        "vector_conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c9b27a",
      "metadata": {
        "id": "29c9b27a"
      },
      "outputs": [],
      "source": [
        "# Model token limits (for context management)\n",
        "MODEL_TOKEN_LIMITS = {\n",
        "    \"gpt-5\": 256000,\n",
        "    \"gpt-5-mini\": 128000,\n",
        "    \"gpt-4o\": 128000,\n",
        "    \"gpt-4o-mini\": 128000,\n",
        "    \"gpt-4-turbo\": 128000,\n",
        "    \"gpt-4\": 8192,\n",
        "    \"gpt-3.5-turbo\": 16385,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ded028dc",
      "metadata": {
        "id": "ded028dc"
      },
      "source": [
        "### Create Conversational Memory Table\n",
        "\n",
        "This function below creates a SQL table to store chat history.\n",
        "\n",
        "Unlike vector stores, conversational memory uses a traditional table because we need exact retrieval by thread ID (not similarity search).\n",
        "\n",
        "**What it does:**\n",
        "- Creates a table with columns: `id`, `thread_id`, `role`, `content`, `timestamp`, `metadata`\n",
        "- Adds an index on `thread_id` for fast conversation lookups\n",
        "- Adds an index on `timestamp` for chronological ordering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54032e53",
      "metadata": {
        "id": "54032e53"
      },
      "outputs": [],
      "source": [
        "def create_conversational_history_table(conn, table_name: str = \"CONVERSATIONAL_MEMORY\"):\n",
        "    \"\"\"\n",
        "    Create a table to store conversational history.\n",
        "\n",
        "    Args:\n",
        "        conn: Oracle database connection\n",
        "        table_name: Name of the table to create\n",
        "    \"\"\"\n",
        "    with conn.cursor() as cur:\n",
        "        # Drop table if exists\n",
        "        try:\n",
        "            cur.execute(f\"DROP TABLE {table_name}\")\n",
        "        except:\n",
        "            pass  # Table doesn't exist\n",
        "\n",
        "        # Create table with proper schema\n",
        "        cur.execute(f\"\"\"\n",
        "            CREATE TABLE {table_name} (\n",
        "                id VARCHAR2(100) DEFAULT SYS_GUID() PRIMARY KEY,\n",
        "                thread_id VARCHAR2(100) NOT NULL,\n",
        "                role VARCHAR2(50) NOT NULL,\n",
        "                content CLOB NOT NULL,\n",
        "                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                metadata CLOB,\n",
        "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                summary_id VARCHAR2(100) DEFAULT NULL\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        # Create index on thread_id for faster lookups\n",
        "        cur.execute(f\"\"\"\n",
        "            CREATE INDEX idx_{table_name.lower()}_thread_id ON {table_name}(thread_id)\n",
        "        \"\"\")\n",
        "\n",
        "        # Create index on timestamp for ordering\n",
        "        cur.execute(f\"\"\"\n",
        "            CREATE INDEX idx_{table_name.lower()}_timestamp ON {table_name}(timestamp)\n",
        "        \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "    print(f\"Table {table_name} created successfully with indexes\")\n",
        "    return table_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24745bff",
      "metadata": {
        "id": "24745bff"
      },
      "outputs": [],
      "source": [
        "# Create the table\n",
        "CONVERSATION_HISTORY_TABLE = create_conversational_history_table(vector_conn, CONVERSATIONAL_TABLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4539b694",
      "metadata": {
        "id": "4539b694"
      },
      "source": [
        "### Create Vector Stores for Each Memory Type\n",
        "\n",
        "Here we create 5 separate vector stores‚Äîone for each memory type.\n",
        "\n",
        "Each vector store is backed by its own Oracle table and uses the same embedding model for consistency.\n",
        "\n",
        "| Vector Store | Purpose |\n",
        "|--------------|---------|\n",
        "| `knowledge_base_vs` | Store documents, facts, and search results |\n",
        "| `workflow_vs` | Store learned action patterns and tool sequences |\n",
        "| `toolbox_vs` | Store tool definitions for semantic tool discovery |\n",
        "| `entity_vs` | Store extracted entities (people, places, systems) |\n",
        "| `summary_vs` | Store compressed summaries for long conversations |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01fffb00",
      "metadata": {
        "id": "01fffb00"
      },
      "outputs": [],
      "source": [
        "knowledge_base_vs = OracleVS(\n",
        "    client=vector_conn,\n",
        "    embedding_function=embedding_model,\n",
        "    table_name=KNOWLEDGE_BASE_TABLE,\n",
        "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
        ")\n",
        "\n",
        "workflow_vs = OracleVS(\n",
        "    client=vector_conn,\n",
        "    embedding_function=embedding_model,\n",
        "    table_name=WORKFLOW_TABLE,\n",
        "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
        ")\n",
        "\n",
        "toolbox_vs = OracleVS(\n",
        "    client=vector_conn,\n",
        "    embedding_function=embedding_model,\n",
        "    table_name=TOOLBOX_TABLE,\n",
        "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
        ")\n",
        "\n",
        "entity_vs = OracleVS(\n",
        "    client=vector_conn,\n",
        "    embedding_function=embedding_model,\n",
        "    table_name=ENTITY_TABLE,\n",
        "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
        ")\n",
        "\n",
        "summary_vs = OracleVS(\n",
        "    client=vector_conn,\n",
        "    embedding_function=embedding_model,\n",
        "    table_name=SUMMARY_TABLE,\n",
        "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75548e90",
      "metadata": {
        "id": "75548e90"
      },
      "source": [
        "Then we create indexes for each of the vector stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0100e3",
      "metadata": {
        "id": "fe0100e3"
      },
      "outputs": [],
      "source": [
        "print(\"Creating vector indexes...\")\n",
        "safe_create_index(vector_conn, knowledge_base_vs, \"knowledge_base_vs_ivf\")\n",
        "safe_create_index(vector_conn, workflow_vs, \"workflow_vs_ivf\")\n",
        "safe_create_index(vector_conn, toolbox_vs, \"toolbox_vs_ivf\")\n",
        "safe_create_index(vector_conn, entity_vs, \"entity_vs_ivf\")\n",
        "safe_create_index(vector_conn, summary_vs, \"summary_vs_ivf\")\n",
        "print(\"All indexes created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929d5ff9",
      "metadata": {
        "id": "929d5ff9"
      },
      "source": [
        "## Programmatic vs Agentic Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc5678a3",
      "metadata": {
        "id": "fc5678a3"
      },
      "source": [
        "A key design decision in memory engineering is determining which operations should be **programmatic** (executed automatically by code) versus **agentic** (decided by the LLM at runtime).\n",
        "\n",
        "| Operation | Programmatic | Agentic |\n",
        "|-----------|:------------:|:-------:|\n",
        "| `read_conversational_memory()` | ‚úÖ | ‚ùå |\n",
        "| `read_knowledge_base()` | ‚úÖ | ‚ùå |\n",
        "| `read_workflow()` | ‚úÖ | ‚ùå |\n",
        "| `read_entity()` | ‚úÖ | ‚ùå |\n",
        "| `read_summary_context()` | ‚úÖ | ‚ùå |\n",
        "| `write_conversational_memory()` | ‚úÖ | ‚ùå |\n",
        "| `write_workflow()` | ‚úÖ | ‚ùå |\n",
        "| `write_entity()` | ‚úÖ | ‚ùå |\n",
        "| `search_tavily()` | ‚ùå | ‚úÖ |\n",
        "| `expand_summary()` | ‚ùå | ‚úÖ |\n",
        "| `summarize_and_store()` | ‚ùå | ‚úÖ |\n",
        "\n",
        "### Why Memory Reads are Programmatic\n",
        "\n",
        "Memory retrieval operations are **always executed** at the start of each agent loop because:\n",
        "\n",
        "1. **Context is essential** ‚Äî The agent needs memory to understand the conversation and avoid repeating mistakes. Without this, every interaction starts from scratch.\n",
        "\n",
        "2. **The agent can't know what it doesn't know** ‚Äî If the agent had to decide whether to check memory, it would need to already know what's in memory‚Äîa chicken-and-egg problem.\n",
        "\n",
        "3. **Consistency** ‚Äî Always loading memory ensures the agent has a predictable, complete view of its knowledge.\n",
        "\n",
        "### Why Memory Writes are Programmatic\n",
        "\n",
        "Storing conversations, workflows, and entities happens automatically because:\n",
        "\n",
        "1. **Reliability** ‚Äî We don't want the agent to \"forget\" to save important information. Conversation history must be persisted consistently.\n",
        "\n",
        "2. **Completeness** ‚Äî Every interaction should be recorded. Selective saving would create gaps in memory.\n",
        "\n",
        "3. **Reduced cognitive load** ‚Äî Letting the agent focus on the task rather than memory management leads to better responses.\n",
        "\n",
        "### Why Tool Calls are Agentic\n",
        "\n",
        "External actions like web search and summary expansion are left to the agent's discretion because:\n",
        "\n",
        "1. **Intent matters** ‚Äî Only the agent knows if it needs more information. Automatically searching for every query would be wasteful.\n",
        "\n",
        "2. **Cost considerations** ‚Äî External API calls have latency and may incur costs. The agent should only call them when genuinely needed.\n",
        "\n",
        "3. **Judgment required** ‚Äî Deciding *what* to search for or *which* summary to expand requires understanding the user's intent‚Äîsomething the LLM excels at."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f3f57d",
      "metadata": {
        "id": "76f3f57d"
      },
      "source": [
        "## Memory Manager Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85d87a8",
      "metadata": {
        "id": "a85d87a8"
      },
      "source": [
        "The `MemoryManager` class is the central abstraction that unifies all memory operations. It provides a clean interface for reading and writing to different memory types, hiding the complexity of SQL queries and vector store operations.\n",
        "\n",
        "### What We're Building\n",
        "\n",
        "A single class that manages 6 types of memory with consistent read/write patterns:\n",
        "\n",
        "| Memory Type | Storage | Write Method | Read Method |\n",
        "|-------------|---------|--------------|-------------|\n",
        "| **Conversational** | SQL Table | `write_conversational_memory()` | `read_conversational_memory()` |\n",
        "| **Knowledge Base** | Vector Store | `write_knowledge_base()` | `read_knowledge_base()` |\n",
        "| **Workflow** | Vector Store | `write_workflow()` | `read_workflow()` |\n",
        "| **Toolbox** | Vector Store | `write_toolbox()` | `read_toolbox()` |\n",
        "| **Entity** | Vector Store | `write_entity()` | `read_entity()` |\n",
        "| **Summary** | Vector Store | `write_summary()` | `read_summary_memory()`, `read_summary_context()` |\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Thread-based conversations** ‚Äî Messages are organized by `thread_id` for multi-conversation support\n",
        "- **Semantic search** ‚Äî Vector stores enable finding relevant content by meaning, not just keywords\n",
        "- **Metadata filtering** ‚Äî Workflows filter by `num_steps > 0`, summaries filter by `id`\n",
        "- **LLM-powered entity extraction** ‚Äî Automatically extracts people, places, and systems from text\n",
        "- **Formatted context output** ‚Äî Each read method returns formatted text ready for the LLM context\n",
        "\n",
        "### Alternative: Memory Manager Frameworks\n",
        "\n",
        "There are existing frameworks that abstract memory management for AI agents:\n",
        "\n",
        "| Framework | Description |\n",
        "|-----------|-------------|\n",
        "| **LangChain Memory** | Built-in memory classes (ConversationBufferMemory, VectorStoreRetrieverMemory) |\n",
        "| **Mem0** | Dedicated memory layer for AI agents with automatic memory management |\n",
        "| **LlamaIndex** | Document-based memory with various storage backends |\n",
        "| **Zep** | Long-term memory service for AI assistants |\n",
        "\n",
        "### Pros and Cons of Building Your Own\n",
        "\n",
        "| Approach | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| **Custom (what we're doing)** | Full control, tailored to your needs, deeper understanding, no external dependencies | More code to maintain, need to handle edge cases yourself |\n",
        "| **Using a framework** | Faster to implement, battle-tested, community support, handles edge cases | Less control, may not fit your exact use case, additional dependency |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69bb7a2f",
      "metadata": {
        "id": "69bb7a2f"
      },
      "source": [
        "> **For learning purposes**, building your own memory manager (as we do here) gives you a deep understanding of how memory engineering works.\n",
        ">\n",
        "> **For production**, you might consider using or extending an existing framework.\n",
        ">\n",
        "> For example, this simple notebook only illustrates reads and writes, but not deletion and updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27aacfa9",
      "metadata": {
        "id": "27aacfa9"
      },
      "outputs": [],
      "source": [
        "import json as json_lib\n",
        "from datetime import datetime\n",
        "\n",
        "class MemoryManager:\n",
        "    \"\"\"\n",
        "    A simplified memory manager for AI agents using Oracle AI Database.\n",
        "\n",
        "    Manages 5 types of memory:\n",
        "    - Conversational: Chat history per thread (SQL table)\n",
        "    - Knowledge Base: Searchable documents (Vector store)\n",
        "    - Workflow: Execution patterns (Vector store)\n",
        "    - Toolbox: Available tools (Vector store)\n",
        "    - Entity: People, places, systems (Vector store)\n",
        "    - Summary: Storing compressed context window\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, conn, conversation_table: str, knowledge_base_vs, workflow_vs, toolbox_vs, entity_vs, summary_vs):\n",
        "        self.conn = conn\n",
        "        self.conversation_table = conversation_table\n",
        "        self.knowledge_base_vs = knowledge_base_vs\n",
        "        self.workflow_vs = workflow_vs\n",
        "        self.toolbox_vs = toolbox_vs\n",
        "        self.entity_vs = entity_vs\n",
        "        self.summary_vs = summary_vs\n",
        "\n",
        "    # ==================== CONVERSATIONAL MEMORY (SQL) ====================\n",
        "\n",
        "    def write_conversational_memory(self, content: str, role: str, thread_id: str) -> str:\n",
        "        \"\"\"Store a message in conversation history.\"\"\"\n",
        "        thread_id = str(thread_id)\n",
        "        with self.conn.cursor() as cur:\n",
        "            id_var = cur.var(str)\n",
        "            cur.execute(f\"\"\"\n",
        "                INSERT INTO {self.conversation_table} (thread_id, role, content, metadata, timestamp)\n",
        "                VALUES (:thread_id, :role, :content, :metadata, CURRENT_TIMESTAMP)\n",
        "                RETURNING id INTO :id\n",
        "            \"\"\", {\"thread_id\": thread_id, \"role\": role, \"content\": content, \"metadata\": \"{}\", \"id\": id_var})\n",
        "            record_id = id_var.getvalue()[0] if id_var.getvalue() else None\n",
        "        self.conn.commit()\n",
        "        return record_id\n",
        "\n",
        "    def read_conversational_memory(self, thread_id: str, limit: int = 10) -> str:\n",
        "        \"\"\"Read conversation history for a thread (excludes summarized messages).\"\"\"\n",
        "        thread_id = str(thread_id)\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(f\"\"\"\n",
        "                SELECT role, content, timestamp FROM {self.conversation_table}\n",
        "                WHERE thread_id = :thread_id AND summary_id IS NULL\n",
        "                ORDER BY timestamp ASC\n",
        "                FETCH FIRST :limit ROWS ONLY\n",
        "            \"\"\", {\"thread_id\": thread_id, \"limit\": limit})\n",
        "            results = cur.fetchall()\n",
        "\n",
        "        messages = [f\"[{ts.strftime('%H:%M:%S')}] [{role}] {content}\" for role, content, ts in results]\n",
        "        messages_formatted = '\\n'.join(messages)\n",
        "        return f\"\"\"## Conversation Memory: This is the conversation history for the current thread\n",
        "### How to use: Use the conversation history to answer the question\n",
        "\n",
        "{messages_formatted}\"\"\"\n",
        "\n",
        "    def mark_as_summarized(self, thread_id: str, summary_id: str):\n",
        "        \"\"\"Mark all unsummarized messages in a thread as summarized.\"\"\"\n",
        "        thread_id = str(thread_id)\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(f\"\"\"\n",
        "                UPDATE {self.conversation_table}\n",
        "                SET summary_id = :summary_id\n",
        "                WHERE thread_id = :thread_id AND summary_id IS NULL\n",
        "            \"\"\", {\"summary_id\": summary_id, \"thread_id\": thread_id})\n",
        "        self.conn.commit()\n",
        "        print(f\"  üì¶ Marked messages as summarized (summary_id: {summary_id})\")\n",
        "\n",
        "    # ==================== KNOWLEDGE BASE (Vector Store) ====================\n",
        "\n",
        "    def write_knowledge_base(self, text: str, metadata: dict):\n",
        "        \"\"\"Store text in knowledge base with metadata.\"\"\"\n",
        "        self.knowledge_base_vs.add_texts([text], [metadata])\n",
        "\n",
        "    def read_knowledge_base(self, query: str, k: int = 3) -> str:\n",
        "        \"\"\"Search knowledge base for relevant content.\"\"\"\n",
        "        results = self.knowledge_base_vs.similarity_search(query, k=k)\n",
        "        content = \"\\n\".join([doc.page_content for doc in results])\n",
        "        return f\"\"\"## Knowledge Base Memory: This are general information that is relevant to the question\n",
        "### How to use: Use the knowledge base as background information that can help answer the question\n",
        "\n",
        "{content}\"\"\"\n",
        "\n",
        "\n",
        "    # ==================== WORKFLOW (Vector Store) ====================\n",
        "\n",
        "    def write_workflow(self, query: str, steps: list, final_answer: str, success: bool = True):\n",
        "        \"\"\"Store a completed workflow pattern for future reference.\"\"\"\n",
        "        # Format steps as text\n",
        "        steps_text = \"\\n\".join([f\"Step {i+1}: {s}\" for i, s in enumerate(steps)])\n",
        "        text = f\"Query: {query}\\nSteps:\\n{steps_text}\\nAnswer: {final_answer[:200]}\"\n",
        "\n",
        "        metadata = {\n",
        "            \"query\": query,\n",
        "            \"success\": success,\n",
        "            \"num_steps\": len(steps),\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        self.workflow_vs.add_texts([text], [metadata])\n",
        "\n",
        "    def read_workflow(self, query: str, k: int = 3) -> str:\n",
        "        \"\"\"Search for similar past workflows with at least 1 step.\"\"\"\n",
        "        # Filter to only include workflows that have steps (num_steps > 0)\n",
        "        results = self.workflow_vs.similarity_search(\n",
        "            query,\n",
        "            k=k,\n",
        "            filter={\"num_steps\": {\"$gt\": 0}}\n",
        "        )\n",
        "        if not results:\n",
        "            return \"## Workflow Memory\\nNo relevant workflows found.\"\n",
        "        content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
        "        return f\"\"\"## Workflow Memory: This are the past workflows that are relevant to the question\n",
        "### How to use: Use the steps and use them to answer the question, especially when using tools and external sources\n",
        "\n",
        "{content}\"\"\"\n",
        "\n",
        "    # ==================== TOOLBOX (Vector Store) ====================\n",
        "\n",
        "    def write_toolbox(self, text: str, metadata: dict):\n",
        "        \"\"\"Store a tool definition in the toolbox.\"\"\"\n",
        "        self.toolbox_vs.add_texts([text], [metadata])\n",
        "\n",
        "    def read_toolbox(self, query: str, k: int = 3) -> list[dict]:\n",
        "        \"\"\"Find relevant tools and return OpenAI-compatible schemas.\"\"\"\n",
        "        results = self.toolbox_vs.similarity_search(query, k=k)\n",
        "        tools = []\n",
        "        for doc in results:\n",
        "            meta = doc.metadata\n",
        "            # Extract parameters from metadata and convert to OpenAI format\n",
        "            stored_params = meta.get(\"parameters\", {})\n",
        "            properties = {}\n",
        "            required = []\n",
        "\n",
        "            for param_name, param_info in stored_params.items():\n",
        "                # Convert stored param info to OpenAI schema format\n",
        "                param_type = param_info.get(\"type\", \"string\")\n",
        "                # Map Python types to JSON schema types\n",
        "                type_mapping = {\n",
        "                    \"<class 'str'>\": \"string\",\n",
        "                    \"<class 'int'>\": \"integer\",\n",
        "                    \"<class 'float'>\": \"number\",\n",
        "                    \"<class 'bool'>\": \"boolean\",\n",
        "                    \"str\": \"string\",\n",
        "                    \"int\": \"integer\",\n",
        "                    \"float\": \"number\",\n",
        "                    \"bool\": \"boolean\"\n",
        "                }\n",
        "                json_type = type_mapping.get(param_type, \"string\")\n",
        "                properties[param_name] = {\"type\": json_type}\n",
        "\n",
        "                # If no default, it's required\n",
        "                if \"default\" not in param_info:\n",
        "                    required.append(param_name)\n",
        "\n",
        "            tools.append({\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": meta.get(\"name\", \"tool\"),\n",
        "                    \"description\": meta.get(\"description\", \"\"),\n",
        "                    \"parameters\": {\"type\": \"object\", \"properties\": properties, \"required\": required}\n",
        "                }\n",
        "            })\n",
        "        return tools\n",
        "\n",
        "    # ==================== ENTITY (Vector Store) ====================\n",
        "\n",
        "    def extract_entities(self, text: str, llm_client) -> list[dict]:\n",
        "        \"\"\"Use LLM to extract entities (people, places, systems) from text.\"\"\"\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return []\n",
        "\n",
        "        prompt = f'''Extract entities from: \"{text[:500]}\"\n",
        "Return JSON: [{{\"name\": \"X\", \"type\": \"PERSON|PLACE|SYSTEM\", \"description\": \"brief\"}}]\n",
        "If none: []'''\n",
        "\n",
        "        try:\n",
        "            response = llm_client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0,\n",
        "                max_tokens=300\n",
        "            )\n",
        "            result = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Extract JSON array from response\n",
        "            start, end = result.find(\"[\"), result.rfind(\"]\")\n",
        "            if start == -1 or end == -1:\n",
        "                return []\n",
        "\n",
        "            parsed = json_lib.loads(result[start:end+1])\n",
        "            return [{\"name\": e[\"name\"], \"type\": e.get(\"type\", \"UNKNOWN\"), \"description\": e.get(\"description\", \"\")}\n",
        "                    for e in parsed if isinstance(e, dict) and e.get(\"name\")]\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def write_entity(self, name: str, entity_type: str, description: str, llm_client=None, text: str = None):\n",
        "        \"\"\"Store an entity OR extract and store entities from text.\"\"\"\n",
        "        if text and llm_client:\n",
        "            # Extract and store entities from text\n",
        "            entities = self.extract_entities(text, llm_client)\n",
        "            for e in entities:\n",
        "                self.entity_vs.add_texts(\n",
        "                    [f\"{e['name']} ({e['type']}): {e['description']}\"],\n",
        "                    [{\"name\": e['name'], \"type\": e['type'], \"description\": e['description']}]\n",
        "                )\n",
        "            return entities\n",
        "        else:\n",
        "            # Store single entity directly\n",
        "            self.entity_vs.add_texts(\n",
        "                [f\"{name} ({entity_type}): {description}\"],\n",
        "                [{\"name\": name, \"type\": entity_type, \"description\": description}]\n",
        "            )\n",
        "\n",
        "    def read_entity(self, query: str, k: int = 5) -> str:\n",
        "        \"\"\"Search for relevant entities.\"\"\"\n",
        "        results = self.entity_vs.similarity_search(query, k=k)\n",
        "        if not results:\n",
        "            return \"## Entity Memory\\nNo entities found.\"\n",
        "\n",
        "        entities = [f\"‚Ä¢ {doc.metadata.get('name', '?')}: {doc.metadata.get('description', '')}\"\n",
        "                    for doc in results if hasattr(doc, 'metadata')]\n",
        "        entities_formatted = '\\n'.join(entities)\n",
        "        return f\"\"\"## Entity Memory: This are the entities that are relevant to the question\n",
        "### How to use: Use the entities to answer the question, especially when having long conversations\n",
        "\n",
        "{entities_formatted}\"\"\"\n",
        "\n",
        "    # ==================== SUMMARY (Vector Store) ====================\n",
        "\n",
        "    def write_summary(self, summary_id: str, full_content: str, summary: str, description: str):\n",
        "        \"\"\"Store a summary with its original content.\"\"\"\n",
        "        self.summary_vs.add_texts(\n",
        "            [f\"{summary_id}: {description}\"],\n",
        "            [{\"id\": summary_id, \"full_content\": full_content, \"summary\": summary, \"description\": description}]\n",
        "        )\n",
        "        return summary_id\n",
        "\n",
        "    def read_summary_memory(self, summary_id: str) -> str:\n",
        "        \"\"\"Retrieve a specific summary by ID (just-in-time retrieval).\"\"\"\n",
        "        results = self.summary_vs.similarity_search(\n",
        "            summary_id,\n",
        "            k=5,\n",
        "            filter={\"id\": summary_id}\n",
        "        )\n",
        "        if not results:\n",
        "            return f\"Summary {summary_id} not found.\"\n",
        "        doc = results[0]\n",
        "        return doc.metadata.get('summary', 'No summary content.')\n",
        "\n",
        "    def read_summary_context(self, query: str = \"\", k: int = 10) -> str:\n",
        "        \"\"\"Get available summaries for context window (IDs + descriptions only).\"\"\"\n",
        "        results = self.summary_vs.similarity_search(query or \"summary\", k=k)\n",
        "        if not results:\n",
        "            return \"## Summary Memory\\nNo summaries available.\"\n",
        "\n",
        "        lines = [\"## Summary Memory\", \"Use expand_summary(id) to get full content:\"]\n",
        "        for doc in results:\n",
        "            sid = doc.metadata.get('id', '?')\n",
        "            desc = doc.metadata.get('description', 'No description')\n",
        "            lines.append(f\"  ‚Ä¢ [ID: {sid}] {desc}\")\n",
        "        return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b775acc2",
      "metadata": {
        "id": "b775acc2"
      },
      "outputs": [],
      "source": [
        "# Initialize the MemoryLayer instance\n",
        "# Note: Uses SQL table for conversational memory, vector stores for others\n",
        "memory_manager = MemoryManager(\n",
        "    conn=vector_conn,\n",
        "    conversation_table=CONVERSATION_HISTORY_TABLE,\n",
        "    knowledge_base_vs=knowledge_base_vs,\n",
        "    workflow_vs=workflow_vs,\n",
        "    toolbox_vs=toolbox_vs,\n",
        "    entity_vs=entity_vs,\n",
        "    summary_vs=summary_vs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1879e58",
      "metadata": {
        "id": "a1879e58"
      },
      "source": [
        "## Creating the Agent's Toolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c0b03d9",
      "metadata": {
        "id": "8c0b03d9"
      },
      "source": [
        "### The Scalability Problem with Tools\n",
        "\n",
        "As your AI system grows, you might have **hundreds of tools** available‚ÄîAPIs, database queries, calculators, search engines, and more. However, passing all tools to the LLM at inference time creates serious problems:\n",
        "\n",
        "| Problem | Impact |\n",
        "|---------|--------|\n",
        "| **Context bloat** | Tool definitions consume tokens, leaving less room for actual content |\n",
        "| **Tool selection failure** | LLMs struggle to choose the right tool when presented with too many options |\n",
        "| **Increased latency** | More tokens = slower inference |\n",
        "| **Higher costs** | More tokens = higher API costs |\n",
        "\n",
        "Model providers like OpenAI and Anthropic typically recommend limiting the number of tools exposed to an LLM (often 10-20 max for reliable selection).\n",
        "\n",
        "### The Solution: Semantic Tool Retrieval\n",
        "\n",
        "The `Toolbox` class solves this by treating tools as a **searchable memory**:\n",
        "\n",
        "1. **Register hundreds of tools** ‚Äî Store all available tools with their descriptions and embeddings\n",
        "2. **Retrieve only relevant tools** ‚Äî At inference time, use vector search to find tools semantically relevant to the current query\n",
        "3. **Pass a focused toolset** ‚Äî Only the retrieved tools (typically 3-5) are passed to the LLM\n",
        "\n",
        "This approach means your system can **scale to hundreds of tools** while the LLM only sees the most relevant ones for each query.\n",
        "\n",
        "### How the Code Works\n",
        "\n",
        "The `Toolbox` class uses **docstrings as the retrieval key**:\n",
        "\n",
        "```\n",
        "User Query ‚Üí Embed Query ‚Üí Vector Search ‚Üí Find tools with similar docstrings ‚Üí Return relevant tools\n",
        "```\n",
        "\n",
        "| Component | Purpose |\n",
        "|-----------|---------|\n",
        "| `get_embedding()` | Converts tool description to a vector |\n",
        "| `ToolMetadata` | Pydantic model storing tool name, description, signature, parameters |\n",
        "| `_augment_docstring()` | Uses LLM to improve the docstring for better retrieval |\n",
        "| `_generate_queries()` | Creates synthetic queries that would trigger this tool |\n",
        "| `register_tool()` | Decorator that stores tool with its embedding in the toolbox |\n",
        "\n",
        "When you call `memory_manager.read_toolbox(query)`, it performs a similarity search to find tools whose docstrings are semantically similar to the query.\n",
        "\n",
        "### The Intersection of Three Engineering Disciplines\n",
        "\n",
        "This implementation combines techniques from **memory engineering**, **context engineering**, and **prompt engineering**:\n",
        "\n",
        "| Discipline | Technique Used | How It Helps |\n",
        "|------------|----------------|--------------|\n",
        "| **Memory Engineering** | Toolbox as procedural memory | Tools are stored and retrieved like learned skills |\n",
        "| **Memory Engineering** | Docstring augmentation | LLM improves docstrings for better semantic retrieval |\n",
        "| **Memory Engineering** | Synthetic query generation | Creates example queries to improve tool discoverability |\n",
        "| **Context Engineering** | Selective tool retrieval | Only relevant tools enter the context, reducing bloat |\n",
        "| **Context Engineering** | Context offloading | Tool results can be summarized to save context space |\n",
        "| **Prompt Engineering** | Role setting | \"You are a technical writer\" improves docstring quality |\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "The `augment=True` flag in `@toolbox.register_tool(augment=True)` triggers:\n",
        "1. **Docstring augmentation** ‚Äî LLM rewrites the docstring to be clearer and more searchable\n",
        "2. **Synthetic query generation** ‚Äî LLM generates example queries that would need this tool\n",
        "3. **Rich embedding** ‚Äî Combines name + augmented docstring + signature + queries for better retrieval\n",
        "\n",
        "This means a simple one-line docstring like `\"Search the web\"` becomes a rich, detailed description that's much more likely to be retrieved when the user asks something like `\"What's the latest news about AI?\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d418dd1",
      "metadata": {
        "id": "8d418dd1"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import uuid\n",
        "from typing import Callable, Optional, Union\n",
        "from pydantic import BaseModel\n",
        "\n",
        "def get_embedding(text: str) -> list[float]:\n",
        "    \"\"\"\n",
        "    Get the embedding for a text using the configured embedding model.\n",
        "    \"\"\"\n",
        "    return embedding_model.embed_query(text)\n",
        "\n",
        "\n",
        "class ToolMetadata(BaseModel):\n",
        "    \"\"\"Metadata for a registered tool.\"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    signature: str\n",
        "    parameters: dict\n",
        "    return_type: str\n",
        "\n",
        "\n",
        "class Toolbox:\n",
        "    \"\"\"\n",
        "    A toolbox for registering, storing, and retrieving tools with LLM-powered augmentation.\n",
        "\n",
        "    Tools are stored with embeddings for semantic retrieval, allowing the agent to\n",
        "    find relevant tools based on natural language queries.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, memory_manager, llm_client, model: str = \"gpt-4o-mini\"):\n",
        "        \"\"\"\n",
        "        Initialize the Toolbox.\n",
        "\n",
        "        Args:\n",
        "            memory_manager: MemoryManager instance for storing tools\n",
        "            llm_client: OpenAI client for LLM augmentation\n",
        "            model: Model to use for augmentation (default: gpt-4o-mini)\n",
        "        \"\"\"\n",
        "        self.memory_manager = memory_manager\n",
        "        self.llm_client = llm_client\n",
        "        self.model = model\n",
        "        self._tools: dict[str, Callable] = {}  # Maps tool_id -> callable\n",
        "        self._tools_by_name: dict[str, Callable] = {}  # Maps function_name -> callable for execution\n",
        "\n",
        "    def _augment_docstring(self, docstring: str) -> str:\n",
        "        \"\"\"\n",
        "        Use LLM to improve and expand a tool's docstring.\n",
        "\n",
        "        Takes a basic docstring and returns an enhanced version with:\n",
        "        - Clearer description of what the tool does\n",
        "        - Better formatted parameters and return values\n",
        "        - Usage examples and edge cases\n",
        "\n",
        "        Args:\n",
        "            docstring: The original docstring to augment\n",
        "\n",
        "        Returns:\n",
        "            An improved, more detailed docstring\n",
        "        \"\"\"\n",
        "        if not docstring.strip():\n",
        "            return \"No description provided.\"\n",
        "\n",
        "\n",
        "        # NOTE: The role description of a technical writer below is a prompt engineering technique that is used to improve the quality of the docstring\n",
        "        # Athough there are research that suggest that role description doesn't realy affect the quality of the LLM's output, it is still a useful technique\n",
        "        # and it is a good [prompt engineering] technique to know.\n",
        "        prompt = f\"\"\"You are a technical writer. Improve the following function docstring to be more clear,\n",
        "            comprehensive, and useful. Include:\n",
        "            1. A clear concise summary\n",
        "            2. Detailed description of what the function does\n",
        "            3. When to use this function\n",
        "            4. Any important notes or caveats\n",
        "\n",
        "            Original docstring:\n",
        "            {docstring}\n",
        "\n",
        "            Return ONLY the improved docstring, no other text.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    def _generate_queries(self, docstring: str, num_queries: int = 5) -> list[str]:\n",
        "        \"\"\"\n",
        "        Generate synthetic example queries that would lead to using this tool.\n",
        "\n",
        "        These queries are used to improve retrieval - by embedding both the tool\n",
        "        description AND example queries, we increase the chances of finding the\n",
        "        right tool when the user asks a related question.\n",
        "\n",
        "        Args:\n",
        "            docstring: The tool's docstring (ideally augmented)\n",
        "            num_queries: Number of example queries to generate\n",
        "\n",
        "        Returns:\n",
        "            List of example natural language queries\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Based on the following tool description, generate {num_queries} diverse example queries\n",
        "            that a user might ask when they need this tool. Make them natural and varied.\n",
        "\n",
        "            Tool description:\n",
        "            {docstring}\n",
        "\n",
        "            Return ONLY a JSON array of strings, like: [\"query1\", \"query2\", ...]\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=300\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            import json\n",
        "            queries = json.loads(response.choices[0].message.content.strip())\n",
        "            return queries if isinstance(queries, list) else []\n",
        "        except json.JSONDecodeError:\n",
        "            # Fallback: extract queries from text\n",
        "            return [response.choices[0].message.content.strip()]\n",
        "\n",
        "    def _get_tool_metadata(self, func: Callable) -> ToolMetadata:\n",
        "        \"\"\"\n",
        "        Extract metadata from a function for storage and retrieval.\n",
        "\n",
        "        Args:\n",
        "            func: The function to extract metadata from\n",
        "\n",
        "        Returns:\n",
        "            ToolMetadata object with function details\n",
        "        \"\"\"\n",
        "        sig = inspect.signature(func)\n",
        "\n",
        "        # Extract parameter info\n",
        "        parameters = {}\n",
        "        for name, param in sig.parameters.items():\n",
        "            param_info = {\"name\": name}\n",
        "            if param.annotation != inspect.Parameter.empty:\n",
        "                param_info[\"type\"] = str(param.annotation)\n",
        "            if param.default != inspect.Parameter.empty:\n",
        "                param_info[\"default\"] = str(param.default)\n",
        "            parameters[name] = param_info\n",
        "\n",
        "        # Extract return type\n",
        "        return_type = \"Any\"\n",
        "        if sig.return_annotation != inspect.Signature.empty:\n",
        "            return_type = str(sig.return_annotation)\n",
        "\n",
        "        return ToolMetadata(\n",
        "            name=func.__name__,\n",
        "            description=func.__doc__ or \"No description\",\n",
        "            signature=str(sig),\n",
        "            parameters=parameters,\n",
        "            return_type=return_type\n",
        "        )\n",
        "\n",
        "    def register_tool(\n",
        "        self, func: Optional[Callable] = None, augment: bool = False\n",
        "    ) -> Union[str, Callable]:\n",
        "        \"\"\"\n",
        "        Register a function as a tool in the toolbox.\n",
        "\n",
        "        Can be used as a decorator or called directly:\n",
        "\n",
        "            @toolbox.register_tool\n",
        "            def my_tool(): ...\n",
        "\n",
        "            @toolbox.register_tool(augment=True)\n",
        "            def my_enhanced_tool(): ...\n",
        "\n",
        "            tool_id = toolbox.register_tool(some_function)\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        func : Callable, optional\n",
        "            The function to register as a tool. If None, returns a decorator.\n",
        "        augment : bool, optional\n",
        "            Whether to augment the tool docstring and generate synthetic queries\n",
        "            using the configured LLM provider.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        Union[str, Callable]\n",
        "            If func is provided, returns the tool ID. Otherwise returns a decorator.\n",
        "        \"\"\"\n",
        "\n",
        "        def decorator(f: Callable) -> str:\n",
        "            docstring = f.__doc__ or \"\"\n",
        "            signature = str(inspect.signature(f))\n",
        "            object_id = uuid.uuid4()\n",
        "            object_id_str = str(object_id)\n",
        "\n",
        "            # NOTE: Augmentation is a technique that is used to improve the quality of the tool's docstring\n",
        "            # by using the LLM to enhance the tool's discoverability and retrieval this is a [memory engineering] technique\n",
        "            if augment:\n",
        "                # Use LLM to enhance the tool's discoverability\n",
        "                augmented_docstring = self._augment_docstring(docstring)\n",
        "                queries = self._generate_queries(augmented_docstring)\n",
        "\n",
        "                # Create rich embedding text combining all information\n",
        "                embedding_text = f\"{f.__name__} {augmented_docstring} {signature} {' '.join(queries)}\"\n",
        "                embedding = get_embedding(embedding_text)\n",
        "\n",
        "                tool_data = self._get_tool_metadata(f)\n",
        "                tool_data.description = augmented_docstring  # Use augmented description\n",
        "\n",
        "                tool_dict = {\n",
        "                    \"_id\": object_id_str,  # Use string, not UUID object\n",
        "                    \"embedding\": embedding,\n",
        "                    \"queries\": queries,\n",
        "                    \"augmented\": True,\n",
        "                    **tool_data.model_dump(),\n",
        "                }\n",
        "            else:\n",
        "                # Basic registration without augmentation\n",
        "                embedding = get_embedding(f\"{f.__name__} {docstring} {signature}\")\n",
        "                tool_data = self._get_tool_metadata(f)\n",
        "\n",
        "                tool_dict = {\n",
        "                    \"_id\": object_id_str,  # Use string, not UUID object\n",
        "                    \"embedding\": embedding,\n",
        "                    \"augmented\": False,\n",
        "                    **tool_data.model_dump(),\n",
        "                }\n",
        "\n",
        "            # Store the tool in the toolbox memory for retrieval\n",
        "            # The embedding enables semantic search to find relevant tools\n",
        "            self.memory_manager.write_toolbox(\n",
        "                f\"{f.__name__} {docstring} {signature}\",\n",
        "                tool_dict\n",
        "            )\n",
        "\n",
        "            # Keep reference to the callable for execution\n",
        "            self._tools[object_id_str] = f\n",
        "            self._tools_by_name[f.__name__] = f  # Also store by name for easy lookup\n",
        "            return object_id_str\n",
        "\n",
        "        if func is None:\n",
        "            return decorator\n",
        "        return decorator(func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6de020f",
      "metadata": {
        "id": "b6de020f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Function to securely get and set environment variables\n",
        "def set_env_securely(var_name, prompt):\n",
        "    value = getpass.getpass(prompt)\n",
        "    os.environ[var_name] = value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb6bacd",
      "metadata": {
        "id": "7eb6bacd"
      },
      "outputs": [],
      "source": [
        "set_env_securely(\"OPENAI_API_KEY\", \"OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb27c840",
      "metadata": {
        "id": "fb27c840"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Initialize the Toolbox\n",
        "toolbox = Toolbox(memory_manager=memory_manager, llm_client=client)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "305bc1bd",
      "metadata": {
        "id": "305bc1bd"
      },
      "source": [
        "# Context Engineering Techniques\n",
        "\n",
        "--------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4ac84b",
      "metadata": {
        "id": "2e4ac84b"
      },
      "source": [
        "> **Context engineering** refers to the set of strategies for curating and maintaining the optimal set of tokens (information) during LLM inference, including all the other information that may land there outside of the prompts.\n",
        ">\n",
        "> ‚Äî *Anthropic*\n",
        "\n",
        "While memory engineering focuses on *what to store and retrieve*, context engineering focuses on *how to manage what's in the context window right now*. This includes monitoring usage, compressing information, and providing just-in-time access to details.\n",
        "\n",
        "## What This Section Covers\n",
        "\n",
        "| Step | Function | Purpose |\n",
        "|------|----------|---------|\n",
        "| **1. Calculate Usage** | `calculate_context_usage()` | Monitor what % of the context window is used |\n",
        "| **2. Summarize** | `summarise_context_window()` | Compress long content into summaries using LLM |\n",
        "| **3. Offload** | `offload_to_summary()` | Auto-trigger summarization when usage exceeds threshold |\n",
        "| **4. Just-in-Time Retrieval** | `expand_summary()` tool | Let agent expand summaries on demand |\n",
        "\n",
        "**`Just-In-Time (JIT)`** retrieval is the process of fetching only the information needed at the exact moment the agent requires it, based on the current task, query, or reasoning step. Instead of loading pre-computed or pre-cached context upfront, the system dynamically retrieves the minimal, most relevant data on demand, ensuring efficiency and reducing context overload. In the context of agent memory JIT is a retrieval-control strategy where memory access is triggered by the agent‚Äôs current goal, query, or reasoning step. Rather than preloading large histories or the full knowledge base, the system dynamically filters, ranks, and injects only the information that materially influences the next token. This reduces context saturation, improves attention allocation, and increases reasoning fidelity.\n",
        "\n",
        "## The Context Management Flow\n",
        "\n",
        "```\n",
        "Context built ‚Üí Check usage % ‚Üí If >80%: Summarize & offload ‚Üí Store summary with ID\n",
        "                                                              ‚Üì\n",
        "Agent sees: [Summary ID: abc123] Brief description ‚Üê Agent can call expand_summary(\"abc123\") if needed\n",
        "```\n",
        "\n",
        "This approach keeps the context lean while giving the agent access to full details when required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39643d7",
      "metadata": {
        "id": "d39643d7"
      },
      "outputs": [],
      "source": [
        "# Context window calculator - returns percentage used\n",
        "def calculate_context_usage(context: str, model: str = \"gpt-4o-mini\") -> dict:\n",
        "    \"\"\"Calculate context window usage as percentage.\"\"\"\n",
        "    estimated_tokens = len(context) // 4  # ~4 chars per token\n",
        "    max_tokens = MODEL_TOKEN_LIMITS.get(model, 128000)\n",
        "    percentage = (estimated_tokens / max_tokens) * 100\n",
        "    return {\"tokens\": estimated_tokens, \"max\": max_tokens, \"percent\": round(percentage, 1)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6db760",
      "metadata": {
        "id": "ea6db760"
      },
      "outputs": [],
      "source": [
        "# Context summariser - calls LLM and stores summary\n",
        "import uuid\n",
        "\n",
        "def summarise_context_window(content: str, memory_manager, llm_client, model: str = \"gpt-4o-mini\") -> dict:\n",
        "    \"\"\"Summarise content using LLM and store in summary memory.\"\"\"\n",
        "    # Call LLM to summarise\n",
        "    response = llm_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Summarise this in 2-3 sentences:\\n{content[:3000]}\"}],\n",
        "        max_tokens=200\n",
        "    )\n",
        "    summary = response.choices[0].message.content\n",
        "\n",
        "    # Generate one-liner description\n",
        "    desc_response = llm_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Write a 10-word label for: {summary}\"}],\n",
        "        max_tokens=30\n",
        "    )\n",
        "    description = desc_response.choices[0].message.content.strip()\n",
        "\n",
        "    # Store in memory\n",
        "    summary_id = str(uuid.uuid4())[:8]\n",
        "    memory_manager.write_summary(summary_id, content, summary, description)\n",
        "\n",
        "    return {\"id\": summary_id, \"description\": description, \"summary\": summary}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1a7538a",
      "metadata": {
        "id": "b1a7538a"
      },
      "outputs": [],
      "source": [
        "# Context offloader - replaces content with summary reference\n",
        "def offload_to_summary(context: str, memory_manager, llm_client, threshold_percent: float = 80.0) -> tuple:\n",
        "    \"\"\"If context exceeds threshold, summarise and return compacted version.\"\"\"\n",
        "    usage = calculate_context_usage(context)\n",
        "\n",
        "    if usage['percent'] < threshold_percent:\n",
        "        return context, []  # No offload needed\n",
        "\n",
        "    # Summarise the context\n",
        "    result = summarise_context_window(context, memory_manager, llm_client)\n",
        "\n",
        "    # Return compact reference instead of full content\n",
        "    compact = f\"[Summary ID: {result['id']}] {result['description']}\"\n",
        "    return compact, [result]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c1efd3",
      "metadata": {
        "id": "77c1efd3"
      },
      "source": [
        "### Summary Tools & Conversation Compaction\n",
        "\n",
        "Below we register the `expand_summary` and `summarize_and_store` functions as tools the agent can call.\n",
        "\n",
        "#### Design Logic: Why Mark Instead of Delete?\n",
        "\n",
        "When conversation history grows large, we need to reduce context window usage. We had two choices:\n",
        "\n",
        "| Approach | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| **Delete summarized messages** | Simple, immediate space savings | Permanent data loss, can't audit or recover |\n",
        "| **Mark as summarized (our choice)** | Preserves history, reversible, auditable | Slightly more complex queries |\n",
        "\n",
        "**Our intuition:** Memory should be *compressed*, or *forgotten* not *erased*. By marking messages with a `summary_id` instead of deleting them:\n",
        "\n",
        "1. **Full history is preserved** ‚Äî Original messages remain in the database for auditing, debugging, or reprocessing\n",
        "2. **Linkage is maintained** ‚Äî Each summary knows which messages it represents (via `summary_id`)\n",
        "3. **Reversible** ‚Äî If a summary is deleted, you could \"unsummarize\" by clearing the `summary_id`\n",
        "\n",
        "#### The Flow\n",
        "\n",
        "```\n",
        "Thread has 50 messages ‚Üí Context too large ‚Üí summarize_conversation(thread_id)\n",
        "                                                    ‚Üì\n",
        "                        1. Read unsummarized messages\n",
        "                        2. LLM summarizes them\n",
        "                        3. Store summary with unique ID\n",
        "                        4. UPDATE messages SET summary_id = 'abc123'\n",
        "                                                    ‚Üì\n",
        "                        Next read: Only new messages appear + Summary ID reference\n",
        "```\n",
        "\n",
        "This is a form of **log compaction** ‚Äî a pattern borrowed from databases and message queues where old entries are compressed but not lost."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aK15jgHmbeZh"
      },
      "id": "aK15jgHmbeZh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54eb2bb6",
      "metadata": {
        "id": "54eb2bb6"
      },
      "outputs": [],
      "source": [
        "# Summary tools for the agent\n",
        "@toolbox.register_tool(augment=True)\n",
        "def expand_summary(summary_id: str) -> str:\n",
        "    \"\"\"Expand a summary reference to full content. Use when you need more details from a [Summary ID: xxx] reference.\"\"\"\n",
        "    return memory_manager.read_summary_memory(summary_id)\n",
        "\n",
        "@toolbox.register_tool(augment=True)\n",
        "def summarize_and_store(text: str, thread_id: str = None) -> str:\n",
        "    \"\"\"Summarize long text and store in memory. Returns a summary ID for later retrieval with expand_summary.\"\"\"\n",
        "    result = summarise_context_window(text, memory_manager, client)\n",
        "    # If thread_id provided, mark conversation messages as summarized\n",
        "    if thread_id:\n",
        "        memory_manager.mark_as_summarized(thread_id, result['id'])\n",
        "    return f\"Stored as [Summary ID: {result['id']}] {result['description']}\"\n",
        "\n",
        "def summarize_conversation(thread_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    Summarize all unsummarized messages in a thread and mark them.\n",
        "    Call this to compact a thread's conversation history.\n",
        "    \"\"\"\n",
        "    # Read current unsummarized messages\n",
        "    conv_memory = memory_manager.read_conversational_memory(thread_id, limit=100)\n",
        "\n",
        "    if not conv_memory or \"[]\" in conv_memory:\n",
        "        return {\"status\": \"nothing_to_summarize\"}\n",
        "\n",
        "    # Summarize the conversation\n",
        "    result = summarise_context_window(conv_memory, memory_manager, client)\n",
        "\n",
        "    # Mark messages as summarized\n",
        "    memory_manager.mark_as_summarized(thread_id, result['id'])\n",
        "\n",
        "    print(f\"‚úÖ Conversation summarized: [Summary ID: {result['id']}]\")\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d05404f5",
      "metadata": {
        "id": "d05404f5"
      },
      "source": [
        "# Web Access with Tavily\n",
        "\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0cce642",
      "metadata": {
        "id": "a0cce642"
      },
      "source": [
        "This section demonstrates how to create an **agentic tool** that the LLM can call to search the web.\n",
        "\n",
        "We use [Tavily](https://tavily.com/), an AI-optimized search API designed for LLM applications.\n",
        "\n",
        "## What This Section Does\n",
        "\n",
        "1. **Initialize the Tavily client** ‚Äî Set up the search API with an API key\n",
        "2. **Register `search_tavily` as a tool** ‚Äî Use `@toolbox.register_tool(augment=True)` to make it discoverable\n",
        "3. **Implement the search-and-store pattern** ‚Äî Results are automatically written to knowledge base memory\n",
        "4. **Test tool retrieval** ‚Äî Verify the tool can be found via semantic search\n",
        "\n",
        "## The Search-and-Store Pattern\n",
        "\n",
        "One thing to note is that not only do we get external context that is not available to the Agent at execution, but we persists this to the knowledge base memory and the Agent can reuse this information in subsequent iteration.\n",
        "When the agent calls `search_tavily()`, it doesn't just return results‚Äîit **persists them to the knowledge base**:\n",
        "\n",
        "```\n",
        "Agent calls search_tavily(\"latest AI news\")\n",
        "       ‚Üì\n",
        "Tavily API returns results\n",
        "       ‚Üì\n",
        "Each result is written to knowledge_base_vs with metadata (title, URL, timestamp)\n",
        "       ‚Üì\n",
        "Future queries can retrieve this information without searching again\n",
        "```\n",
        "\n",
        "This pattern means the agent **learns** from its searches. Information discovered once becomes part of the agent's long-term memory, available for future conversations without additional API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500d7836",
      "metadata": {
        "id": "500d7836"
      },
      "outputs": [],
      "source": [
        "set_env_securely(\"TAVILY_API_KEY\", \"Tavily API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6772460e",
      "metadata": {
        "id": "6772460e"
      },
      "outputs": [],
      "source": [
        "from tavily import TavilyClient\n",
        "from datetime import datetime\n",
        "\n",
        "# Don't forget to set your API key!\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "@toolbox.register_tool(augment=True)\n",
        "def search_tavily(query: str, max_results: int = 5):\n",
        "    \"\"\"\n",
        "    Use this function to search the web and store the results in the knowledge base.\n",
        "    \"\"\"\n",
        "    response = tavily_client.search(query=query, max_results=max_results)\n",
        "    results = response.get(\"results\", [])\n",
        "\n",
        "    # Write each result to the knowledge base\n",
        "    for result in results:\n",
        "        # Create the text content to embed\n",
        "        text = f\"Title: {result.get('title', '')}\\nContent: {result.get('content', '')}\\nURL: {result.get('url', '')}\"\n",
        "\n",
        "        # Create metadata\n",
        "        metadata = {\n",
        "            \"title\": result.get(\"title\", \"\"),\n",
        "            \"url\": result.get(\"url\", \"\"),\n",
        "            \"score\": result.get(\"score\", 0),\n",
        "            \"source_type\": \"tavily_search\",\n",
        "            \"query\": query,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Write to knowledge base\n",
        "        memory_manager.write_knowledge_base(text, metadata)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7f8535",
      "metadata": {
        "id": "7c7f8535"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "retreived_tools = memory_manager.read_toolbox(\"Search the internet\")\n",
        "pprint.pprint(retreived_tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e993b26",
      "metadata": {
        "id": "9e993b26"
      },
      "source": [
        "# Agent Execution\n",
        "\n",
        "--------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e94e836",
      "metadata": {
        "id": "7e94e836"
      },
      "source": [
        "This is where everything comes together. We build a complete **agent loop** that integrates all the memory types, context engineering, and tool calling we've implemented.\n",
        "\n",
        "## What This Section Contains\n",
        "\n",
        "| Component | Purpose |\n",
        "|-----------|---------|\n",
        "| `AGENT_SYSTEM_PROMPT` | Instructions telling the LLM how to use memory and tools |\n",
        "| `execute_tool()` | Looks up and executes tools from the toolbox by name |\n",
        "| `call_openai_chat()` | Wrapper for OpenAI Chat Completions API with tool support |\n",
        "| `call_agent()` | The main agent loop that orchestrates everything |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc1e5e6e",
      "metadata": {
        "id": "bc1e5e6e"
      },
      "outputs": [],
      "source": [
        "import json as json_lib\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# ==================== SYSTEM PROMPT ====================\n",
        "# Below is an example of prompt engineering technique called role description.\n",
        "# It is a technique that is used to improve the quality of the LLM's output.\n",
        "# Although there are research that suggest that role description doesn't realy affect the quality of the LLM's output, it is still a useful technique\n",
        "#¬†and it is a good [prompt engineering] technique to know.\n",
        "AGENT_SYSTEM_PROMPT = \"\"\"\n",
        "# System Instructions\n",
        "You are an intelligent assistant with access to memory and tools.\n",
        "\n",
        "IMPORTANT: The user's input contains CONTEXT that has already been retrieved for you:\n",
        "- Conversation Memory: Previous conversations\n",
        "- Knowledge Base Memory: Relevant documents\n",
        "- Summary Memory: Compressed summaries with IDs\n",
        "\n",
        "## Summary Memory\n",
        "When you see [Summary ID: xxx] entries, you can call expand_summary(summary_id) to get the full content.\n",
        "Use this for just-in-time retrieval when you need more details.\n",
        "\n",
        "When answering:\n",
        "1. FIRST, use the context provided in the input\n",
        "2. If you need more detail from a summary, call expand_summary\n",
        "3. Only use search tools if context is insufficient\n",
        "\"\"\"\n",
        "\n",
        "def execute_tool(tool_name: str, tool_args: dict) -> str:\n",
        "    \"\"\"Execute a tool by looking it up in the toolbox.\"\"\"\n",
        "\n",
        "    if tool_name not in toolbox._tools_by_name:\n",
        "        return f\"Error: Tool '{tool_name}' not found\"\n",
        "\n",
        "    return str(toolbox._tools_by_name[tool_name](**tool_args) or \"Done\")\n",
        "\n",
        "# ==================== OPENAI CHAT FUNCTION ====================\n",
        "def call_openai_chat(messages: list, tools: list = None, model: str = \"gpt-4o-mini\", temperature: float = 0.4):\n",
        "    \"\"\"Call OpenAI Chat Completions API with tools.\"\"\"\n",
        "    kwargs = {\"model\": model, \"messages\": messages, \"temperature\": temperature}\n",
        "    if tools:\n",
        "        kwargs[\"tools\"] = tools\n",
        "        kwargs[\"tool_choice\"] = \"auto\"\n",
        "    return client.chat.completions.create(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f5c797b",
      "metadata": {
        "id": "9f5c797b"
      },
      "source": [
        "## The Agent Loop Flow\n",
        "\n",
        "```\n",
        "1. BUILD CONTEXT\n",
        "   ‚îú‚îÄ‚îÄ Read conversational memory (chat history)\n",
        "   ‚îú‚îÄ‚îÄ Read knowledge base (relevant documents)\n",
        "   ‚îú‚îÄ‚îÄ Read workflow memory (past action patterns)\n",
        "   ‚îú‚îÄ‚îÄ Read entity memory (people, places, systems)\n",
        "   ‚îî‚îÄ‚îÄ Read summary context (available summary IDs)\n",
        "\n",
        "2. CHECK CONTEXT USAGE\n",
        "   ‚îî‚îÄ‚îÄ If >80% used ‚Üí Summarize and offload\n",
        "\n",
        "3. GET TOOLS\n",
        "   ‚îî‚îÄ‚îÄ Retrieve semantically relevant tools from toolbox\n",
        "\n",
        "4. STORE USER MESSAGE\n",
        "   ‚îî‚îÄ‚îÄ Write to conversational memory + extract entities\n",
        "\n",
        "5. AGENT LOOP (up to max_iterations)\n",
        "   ‚îú‚îÄ‚îÄ Call LLM with context + tools\n",
        "   ‚îú‚îÄ‚îÄ If tool calls ‚Üí Execute tools, add results to messages\n",
        "   ‚îî‚îÄ‚îÄ If no tool calls ‚Üí Return final answer\n",
        "\n",
        "6. SAVE RESULTS\n",
        "   ‚îú‚îÄ‚îÄ Write workflow (if tools were used)\n",
        "   ‚îú‚îÄ‚îÄ Extract entities from response\n",
        "   ‚îî‚îÄ‚îÄ Store assistant response in conversational memory\n",
        "```\n",
        "\n",
        "## Key Design Decisions\n",
        "\n",
        "- **Memory is loaded programmatically** ‚Äî The agent always has context without deciding to \"remember\"\n",
        "- **Tools are retrieved semantically** ‚Äî Only relevant tools are passed to the LLM\n",
        "- **Context is monitored** ‚Äî Auto-summarization prevents overflow\n",
        "- **Everything is persisted** ‚Äî Conversations, workflows, and entities are saved for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ddc7b7f",
      "metadata": {
        "id": "5ddc7b7f"
      },
      "outputs": [],
      "source": [
        "# ==================== MAIN AGENT LOOP ====================\n",
        "def call_agent(query: str, thread_id: str = \"1\", max_iterations: int = 10) -> str:\n",
        "    \"\"\"Agent loop with context window monitoring and summarization.\"\"\"\n",
        "    thread_id = str(thread_id)\n",
        "    steps = []\n",
        "    summaries = []  # Track created summaries\n",
        "\n",
        "    # 1. Build context from memory\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üß† BUILDING CONTEXT...\")\n",
        "\n",
        "    context = f\"# Question\\n{query}\\n\\n\"\n",
        "    context += memory_manager.read_conversational_memory(thread_id) + \"\\n\\n\"\n",
        "    context += memory_manager.read_knowledge_base(query) + \"\\n\\n\"\n",
        "    context += memory_manager.read_workflow(query) + \"\\n\\n\"\n",
        "    context += memory_manager.read_entity(query) + \"\\n\\n\"\n",
        "    context += memory_manager.read_summary_context(query) + \"\\n\\n\"  # Shows IDs + descriptions\n",
        "\n",
        "    print(\"====CONTEXT WINDOW=====\\n\")\n",
        "    print(context)\n",
        "\n",
        "    # 2. Check context usage - summarize if >80%\n",
        "    usage = calculate_context_usage(context)\n",
        "    print(f\"üìä Context: {usage['percent']}% ({usage['tokens']}/{usage['max']} tokens)\")\n",
        "\n",
        "    if usage['percent'] > 80:\n",
        "        print(\"‚ö†Ô∏è Context >80% - summarizing...\")\n",
        "        context, summaries = offload_to_summary(context, memory_manager, client)\n",
        "        # Add summary references to context\n",
        "        if summaries:\n",
        "            summary_section = \"\\n## Summary Memory\\n\"\n",
        "            for s in summaries:\n",
        "                summary_section += f\"[Summary ID: {s['id']}] {s['description']}\\n\"\n",
        "            context = summary_section + \"\\n\" + context\n",
        "        usage = calculate_context_usage(context)\n",
        "        print(f\"üìä After summarization: {usage['percent']}%\")\n",
        "\n",
        "    # 3. Get tools\n",
        "    dynamic_tools = memory_manager.read_toolbox(query, k=5)\n",
        "    print(f\"üîß Tools: {[t['function']['name'] for t in dynamic_tools]}\")\n",
        "\n",
        "    # 4. Store user message & extract entities\n",
        "    memory_manager.write_conversational_memory(query, \"user\", thread_id)\n",
        "    try:\n",
        "        memory_manager.write_entity(\"\", \"\", \"\", llm_client=client, text=query)\n",
        "    except: pass\n",
        "\n",
        "    # 5. Agent loop\n",
        "    messages = [{\"role\": \"system\", \"content\": AGENT_SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": context}]\n",
        "    final_answer = \"\"\n",
        "\n",
        "    print(\"\\nü§ñ AGENT LOOP\")\n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
        "\n",
        "        response = call_openai_chat(messages, tools=dynamic_tools)\n",
        "        msg = response.choices[0].message\n",
        "\n",
        "        if msg.tool_calls:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content, \"tool_calls\": [\n",
        "                {\"id\": tc.id, \"type\": \"function\", \"function\": {\"name\": tc.function.name, \"arguments\": tc.function.arguments}}\n",
        "                for tc in msg.tool_calls\n",
        "            ]})\n",
        "\n",
        "            for tc in msg.tool_calls:\n",
        "                tool_name = tc.function.name\n",
        "                tool_args = json_lib.loads(tc.function.arguments)\n",
        "                # Format args for display (truncate long values)\n",
        "                args_display = {k: (v[:50] + '...' if isinstance(v, str) and len(v) > 50 else v)\n",
        "                               for k, v in tool_args.items()}\n",
        "                print(f\"üõ†Ô∏è {tool_name}({args_display})\")\n",
        "\n",
        "                try:\n",
        "                    result = execute_tool(tool_name, tool_args)\n",
        "                    steps.append(f\"{tool_name}({args_display}) ‚Üí success\")\n",
        "                except Exception as e:\n",
        "                    result = f\"Error: {e}\"\n",
        "                    steps.append(f\"{tool_name}({args_display}) ‚Üí failed\")\n",
        "\n",
        "                print(f\"   ‚Üí {result[:200]}...\")\n",
        "                messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": result})\n",
        "        else:\n",
        "            final_answer = msg.content or \"\"\n",
        "            print(f\"\\n‚úÖ DONE ({len(steps)} tool calls)\")\n",
        "            break\n",
        "\n",
        "    # 6. Save workflow & entities\n",
        "    if steps:\n",
        "        memory_manager.write_workflow(query, steps, final_answer)\n",
        "    try:\n",
        "        memory_manager.write_entity(\"\", \"\", \"\", llm_client=client, text=final_answer)\n",
        "    except: pass\n",
        "    memory_manager.write_conversational_memory(final_answer, \"assistant\", thread_id)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + f\"\\nüí¨ ANSWER:\\n{final_answer}\\n\" + \"=\"*50)\n",
        "    return final_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd88f84",
      "metadata": {
        "id": "9dd88f84"
      },
      "outputs": [],
      "source": [
        "call_agent(\"These results are good, can you double click into the most relevant one and tell me more about it?\", thread_id=\"0\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "playground",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}