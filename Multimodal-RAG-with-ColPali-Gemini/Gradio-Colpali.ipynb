{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1d01077",
      "metadata": {
        "id": "e1d01077"
      },
      "source": [
        "<img src = \"https://learnopencv.com/wp-content/uploads/2024/09/Feature-Multimodal-RAG-with-ColPali-Gemini.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e99bdba",
      "metadata": {
        "id": "3e99bdba"
      },
      "source": [
        "#### Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ee3fb7-9d95-45be-9a5f-1a9e139ac004",
      "metadata": {
        "id": "86ee3fb7-9d95-45be-9a5f-1a9e139ac004"
      },
      "outputs": [],
      "source": [
        "!pip install pdf2image einops google-generativeai gradio -q\n",
        "!pip install colpali-engine==0.2.2 -q\n",
        "!pip install -U bitsandbytes -q\n",
        "!pip install mteb transformers tqdm typer seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8bd4f7-3867-4123-b4bd-129a83411c59",
      "metadata": {
        "id": "aa8bd4f7-3867-4123-b4bd-129a83411c59"
      },
      "outputs": [],
      "source": [
        "# Run in terminals\n",
        "sudo apt install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "444067e1-a756-4160-855b-cbfd75c7d8d2",
      "metadata": {
        "id": "444067e1-a756-4160-855b-cbfd75c7d8d2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
        "from colpali_engine.trainer.retrieval_evaluator import CustomEvaluator\n",
        "from colpali_engine.utils.colpali_processing_utils import process_images, process_queries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c589f8c",
      "metadata": {
        "id": "9c589f8c"
      },
      "source": [
        "#### Load Model from HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "452aeda7-865c-43af-9e92-d99912bd2bde",
      "metadata": {
        "id": "452aeda7-865c-43af-9e92-d99912bd2bde"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model_name = \"vidore/colpali\"\n",
        "hf_token = getpass.getpass(\"Enter HF API: \")\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "model = ColPali.from_pretrained(\n",
        "    \"google/paligemma-3b-mix-448\", torch_dtype=torch.bfloat16, device_map=\"cuda\", token=hf_token\n",
        ").eval()\n",
        "model.load_adapter(model_name)\n",
        "processor = AutoProcessor.from_pretrained(model_name, token=hf_token)\n",
        "device = model.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7ab980",
      "metadata": {
        "id": "8b7ab980"
      },
      "source": [
        "### ColPali"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "477d4e32",
      "metadata": {
        "id": "477d4e32"
      },
      "source": [
        "1. Offline Indexing - ColPali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "defe2323-f9b5-4c03-89a4-09198da1b35e",
      "metadata": {
        "id": "defe2323-f9b5-4c03-89a4-09198da1b35e"
      },
      "outputs": [],
      "source": [
        "def index(file, ds):\n",
        "\n",
        "    images = []\n",
        "    for f in file:\n",
        "        images.extend(convert_from_path(f))\n",
        "\n",
        "    # run inference - docs\n",
        "    dataloader = DataLoader(\n",
        "        images,\n",
        "        batch_size=4,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: process_images(processor, x),\n",
        "    )\n",
        "    for batch_doc in tqdm(dataloader):\n",
        "        with torch.no_grad():\n",
        "            batch_doc = {k: v.to(device) for k, v in batch_doc.items()}\n",
        "            embeddings_doc = model(**batch_doc)\n",
        "        ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n",
        "    return f\"Uploaded and converted {len(images)} pages\", ds, images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589d39f2",
      "metadata": {
        "id": "589d39f2"
      },
      "source": [
        "2. Online Querying - ColPali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd03d971",
      "metadata": {
        "id": "cd03d971"
      },
      "outputs": [],
      "source": [
        "def search(query: str, ds, images):\n",
        "    qs = []\n",
        "    with torch.no_grad():\n",
        "        batch_query = process_queries(processor, [query], mock_image)\n",
        "        batch_query = {k: v.to(device) for k, v in batch_query.items()}\n",
        "        embeddings_query = model(**batch_query)\n",
        "        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n",
        "\n",
        "    # run evaluation\n",
        "    retriever_evaluator = CustomEvaluator(is_multi_vector=True)\n",
        "    scores = retriever_evaluator.evaluate(qs, ds)\n",
        "    best_page = int(scores.argmax(axis=1).item())\n",
        "    return f\"The most relevant page is {best_page}\", images[best_page]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ec43c13",
      "metadata": {
        "id": "3ec43c13"
      },
      "source": [
        "### Google Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ca16a6-8cdd-4271-8c49-8a9a16502008",
      "metadata": {
        "id": "03ca16a6-8cdd-4271-8c49-8a9a16502008",
        "outputId": "503b1fa0-e0ca-4a05-ffe9-bcef67589347"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-1.5-flash',\n",
              "    generation_config={'temperature': 0.0, 'top_p': 0.95, 'top_k': 64, 'max_output_tokens': 1024, 'response_mime_type': 'text/plain'},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "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\n",
        "generation_config = {\n",
        "  \"temperature\": 0.0,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 1024,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyC-j70oiSBB-Ta9-6ptqMAYcv4aUVNop0w\")\n",
        "\n",
        "gemini_flash = genai.GenerativeModel(model_name=\"gemini-1.5-flash\" , generation_config=generation_config)\n",
        "\n",
        "def get_answer(prompt:str , image:Image):\n",
        "  response = model.generate_content([prompt, image])\n",
        "  return response.text\n",
        "\n",
        "gemini_flash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a7d642",
      "metadata": {
        "id": "03a7d642"
      },
      "source": [
        "### Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e38cb8c2-f723-4086-86b2-2b4b29c0cb7d",
      "metadata": {
        "id": "e38cb8c2-f723-4086-86b2-2b4b29c0cb7d",
        "outputId": "43c1fa17-18f7-4af8-cb8d-5f7074ec7430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://53354a1cb47d90d401.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://53354a1cb47d90d401.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/gradio/route_utils.py\", line 321, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/gradio/utils.py\", line 826, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3873/1516570879.py\", line 4, in index\n",
            "    for f in file:\n",
            "TypeError: 'NoneType' object is not iterable\n",
            "100%|██████████| 16/16 [00:13<00:00,  1.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([50])\n",
            "Top 1 Accuracy (verif): 0.0\n",
            "tensor([23])\n",
            "Top 1 Accuracy (verif): 0.0\n",
            "tensor([23])\n",
            "Top 1 Accuracy (verif): 0.0\n",
            "tensor([55])\n",
            "Top 1 Accuracy (verif): 0.0\n",
            "tensor([55])\n",
            "Top 1 Accuracy (verif): 0.0\n",
            "tensor([25])\n",
            "Top 1 Accuracy (verif): 0.0\n"
          ]
        }
      ],
      "source": [
        "COLORS = [\"#4285f4\", \"#db4437\", \"#f4b400\", \"#0f9d58\", \"#e48ef1\"]\n",
        "\n",
        "mock_image = Image.new(\"RGB\", (448, 448), (255, 255, 255))\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ColPali: Efficient Document Retrieval with Vision Language Models 📚🔍\")\n",
        "    gr.Markdown(\"## 1️⃣ Upload PDFs\")\n",
        "    file = gr.File(file_types=[\"pdf\"], file_count=\"multiple\")\n",
        "\n",
        "    gr.Markdown(\"## 2️⃣ Index the PDFs and upload\")\n",
        "    convert_button = gr.Button(\"🔄 Convert and upload\")\n",
        "    message = gr.Textbox(\"Files not yet uploaded\")\n",
        "    embeds = gr.State(value=[])\n",
        "    imgs = gr.State(value=[])\n",
        "\n",
        "    # Define the actions for conversion\n",
        "    convert_button.click(index, inputs=[file, embeds], outputs=[message, embeds, imgs])\n",
        "\n",
        "    gr.Markdown(\"## 3️⃣ Search\")\n",
        "    query = gr.Textbox(placeholder=\"Enter your query to match\")\n",
        "    search_button = gr.Button(\"🔍 Search\")\n",
        "\n",
        "    gr.Markdown(\"## 4️⃣ ColPali Retrieval\")\n",
        "    message2 = gr.Textbox(\"Most relevant image is...\")\n",
        "    output_img = gr.Image()\n",
        "\n",
        "    gr.Markdown(\"## 5️⃣ Gemini Response\")\n",
        "    output_text = gr.Textbox(\"Gemini Response...\")\n",
        "\n",
        "    def get_answer(prompt:str , image:Image):\n",
        "       response = gemini_flash.generate_content([prompt, image])\n",
        "       return response.text\n",
        "\n",
        "    # Function to combine retrieval and LLM call\n",
        "    def search_with_llm(query, ds, images, prompt=\"What is shown in this image, analyse and provide some interpretation? Format the answer in a neat 500 words summary.\"):\n",
        "        # Step 1: Search the best image based on query\n",
        "        search_message, best_image = search(query, ds, images)\n",
        "\n",
        "        # Step 2: Generate an answer using LLM\n",
        "        answer = get_answer(prompt, best_image)\n",
        "\n",
        "        return search_message, best_image, answer\n",
        "\n",
        "    # Action for search button\n",
        "    search_button.click(\n",
        "        search_with_llm,\n",
        "        inputs=[query, embeds, imgs],\n",
        "        outputs=[message2, output_img, output_text]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(max_size=10).launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf42cfd1-a7e9-4f91-92ef-e93b1cbb6335",
      "metadata": {
        "id": "cf42cfd1-a7e9-4f91-92ef-e93b1cbb6335"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}